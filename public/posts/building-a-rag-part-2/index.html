<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Journey into building a custom RAG (Day 2) | Cyril LAY</title>
<meta name="keywords" content="">
<meta name="description" content="Now that I have a basic RAG working, I performed a few experiments to assess its accuracy in retrieving relevant documents.
I did an experiment where I took one part of huggingface TRL library that is supposed to be included in my embeddings, regarding DPO (Direct Preference Optimization) :
Then, I asked my model questions about it, but it wouldn&rsquo;t get them right, and outputted something like chatGPT would, without context and knowledge about DPO.">
<meta name="author" content="Cyril LAY">
<link rel="canonical" href="https://blog.lays.pro./posts/building-a-rag-part-2/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.5ff2630c4d1b3e25bc21f0ecd96681dbcf58219e741fa627857820b5485cb770.css" integrity="sha256-X/JjDE0bPiW8IfDs2WaB289YIZ50H6YnhXggtUhct3A=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://blog.lays.pro./favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://blog.lays.pro./favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://blog.lays.pro./favicon-32x32.png">
<link rel="apple-touch-icon" href="https://blog.lays.pro./apple-touch-icon.png">
<link rel="mask-icon" href="https://blog.lays.pro./safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript><meta property="og:title" content="Journey into building a custom RAG (Day 2)" />
<meta property="og:description" content="Now that I have a basic RAG working, I performed a few experiments to assess its accuracy in retrieving relevant documents.
I did an experiment where I took one part of huggingface TRL library that is supposed to be included in my embeddings, regarding DPO (Direct Preference Optimization) :
Then, I asked my model questions about it, but it wouldn&rsquo;t get them right, and outputted something like chatGPT would, without context and knowledge about DPO." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://blog.lays.pro./posts/building-a-rag-part-2/" /><meta property="og:image" content="https://blog.lays.pro./assets/images/mixtral.png"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-02-09T01:23:06+01:00" />
<meta property="article:modified_time" content="2024-02-09T01:23:06+01:00" /><meta property="og:site_name" content="AI &amp; Data blog by Cyril LAY" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://blog.lays.pro./assets/images/mixtral.png"/>

<meta name="twitter:title" content="Journey into building a custom RAG (Day 2)"/>
<meta name="twitter:description" content="Now that I have a basic RAG working, I performed a few experiments to assess its accuracy in retrieving relevant documents.
I did an experiment where I took one part of huggingface TRL library that is supposed to be included in my embeddings, regarding DPO (Direct Preference Optimization) :
Then, I asked my model questions about it, but it wouldn&rsquo;t get them right, and outputted something like chatGPT would, without context and knowledge about DPO."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://blog.lays.pro./posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Journey into building a custom RAG (Day 2)",
      "item": "https://blog.lays.pro./posts/building-a-rag-part-2/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Journey into building a custom RAG (Day 2)",
  "name": "Journey into building a custom RAG (Day 2)",
  "description": "Now that I have a basic RAG working, I performed a few experiments to assess its accuracy in retrieving relevant documents.\nI did an experiment where I took one part of huggingface TRL library that is supposed to be included in my embeddings, regarding DPO (Direct Preference Optimization) :\nThen, I asked my model questions about it, but it wouldn\u0026rsquo;t get them right, and outputted something like chatGPT would, without context and knowledge about DPO.",
  "keywords": [
    
  ],
  "articleBody": "Now that I have a basic RAG working, I performed a few experiments to assess its accuracy in retrieving relevant documents.\nI did an experiment where I took one part of huggingface TRL library that is supposed to be included in my embeddings, regarding DPO (Direct Preference Optimization) :\nThen, I asked my model questions about it, but it wouldnâ€™t get them right, and outputted something like chatGPT would, without context and knowledge about DPO.\nAt this moment, I figured out I am probably going to perform multiple experiments : adjusting the filters, parsing techniques, etc, so I figured I will need a good tool to track the experiments, quickly retrieve the artifacts, etc. I used code from metaflow so my pipeline was already structured as a Metaflow flow, but I didnâ€™t have the associated services required to fully enjoy the features.\nTime to deploy Metaflow service, DB and UI locally with docker so that we can easily push it to the cloud if I need it at some point.\nDeploy metaflow To run the metaflow service (which includes one docker for metadata and one for UI backend) :\ngit clone https://github.com/Netflix/metaflow-service.git \u0026\u0026 cd metaflow-service docker-compose -f docker-compose.development.yml up It runs on the 8083 port by default.\nNow run the metaflow UI front-end, using the endpoint of your just deployed service :\ngit clone https://github.com/Netflix/metaflow-ui/ \u0026\u0026 cd metaflow-ui docker build --tag metaflow-ui:latest . docker run -p 3000:3000 -e METAFLOW_SERVICE=http://localhost:8083/ metaflow-ui:latest It works ðŸ¥³\nNow,I can easily visualize my runs, and if I saved some artifacts during my flow with self.my_df = my_df like :\n@schedule(daily=True) class MarkdownChunker(FlowSpec): @step def parse_clean_docs(self): self.repo_params = config.repo_params.MARKDOWN_REPOS_TO_INDEX self.parsed_filtered_df = self.pre_process_markdown() self.next(self.end) Now, I can eventually inspect one runâ€™s artifacts in a notebook :\nfrom metaflow import Flow flow = Flow(\"MarkdownChunker\") run = flow.latest_successful_run parsed_filtered_df_latest_run = run.data.parsed_filtered_df So, parsed_filtered_df_latest_run is the dataframe of the parsed, clean content to be retrieved in the RAG app. I inspected it and noticed that the DPO part I was looking for, is not in the dataframe, so it was either filtered out, or not scraped at all, so it wonâ€™t exist in the embeddingâ€™s DB and canâ€™t be fed as context to the LLM.\nI now need to iterate on the parsing/filtering logic before embeddings, and figured out it was a good time to configure a proper Vector DB, and use LLamaIndex as an abstraction of the interface between my content and the DB.\nSetup LanceDB I wanted to persist my embeddings so that they are not recomputed each time (it takes CPU/GPU time if done locally, money if done with openAIâ€™s embedding)\nFor now it was persisted on the disk by default, but I want to switch to a proper vector DB :\nmore efficient for querying closer to a production setup I heard from my friend in ML that lanceDB was currently a great choice (open-source, very efficient (written in Rust), easy to use), but if you need to do it yourself, I suggest comparing the available solutions yourself. Hereâ€™s a helpful pdf to help you in this task.\nHere is how to do it :\nCreate the Vector DB vector_store = LanceDBVectorStore(uri=\"./data_lanceDB\") # Switch to a storage_context = StorageContext.from_defaults(vector_store=vector_store) index = VectorStoreIndex.from_vector_store( vector_store, storage_context=storage_context ) Embed your data and insert it in the vector store # Get the latest execution of our markdown processing pipeline with metaflow for _run in Flow('DataTableProcessor'): if _run.data.save_processed_df: run = _run break df = run.data.processed_df vector_store = LanceDBVectorStore(uri=\"./data_lanceDB\") storage_context = StorageContext.from_defaults(vector_store=vector_store) index = VectorStoreIndex.from_documents( [Document(t) for t in df.contents], storage_context=storage_context, ) Under the hood, the last instruction actually embeds our data using the previously defined service_context (See part 1 of this blog series), if defined globally (set_global_service_context(service_context)), or you can also pass it as a service_context=service_context parameter of the VectorStoreIndex.from_documents().\nRetrieve your existing embeddings def load_index(): vector_store = LanceDBVectorStore(uri=\"./data_lanceDB\") storage_context = StorageContext.from_defaults(vector_store=vector_store) index = VectorStoreIndex.from_vector_store( vector_store, storage_context=storage_context ) return index Now that our pipeline is set to use Metaflow, we can save the index (our custom data vectorized in LanceDB) during the flow with self.embeddings_index = index to access it later, at inference time for instance.\nYou can then use it to perform simple retrievals :\nretriever = self.embeddings_index.as_retriever(similarity_top_k=5) query = \"How do I fine-tune a model with HuggingFace DPO ?\" retrieved_nodes = retriever.retrieve(query) retrieved_context = [node.get_text() for node in retrieved_nodes] print(f\"Retrieved context associated with query '{query}' : \\n{retrieved_context}\") Or as a query engine, which will fetch the relevant prompt from our query, and embed it with our query to ask a LLM (any LLM can be configured : openAI, local, Ollama, API, Groqâ€¦) :\nfrom llama_index.llms.huggingface import HuggingFaceLLM llm = HuggingFaceLLM(model_name=\"HuggingFaceH4/zephyr-7b-alpha\") query_engine = self.embeddings_index.as_query_engine(service_context=ServiceContext.from_defaults(llm=llm)) response = query_engine.query(query) print(f\"Response from model for query {query} : \\n{response.response}\") Or as a chat engine, which can handle context and history :\nindex.as_chat_engine( chat_mode=\"condense_plus_context\" context_prompt=\"...Use your personal knowledge AND the context retrieved here {context_str}\" ) Various chat modes exist in LlamaIndex, depending on if you want context or not, how to deal with chat history, etc.\nNote: In some usecases youâ€™ll want to fully disable the LLMâ€™s knowledge, and force it to only use the context, to avoid hallucinations for instance.\nNext Steps Check how the default prompt logic works with LlamaIndex Verify that the model can use both of its existing knowledge, and our embeddings' Switch OpenAIâ€™s gpt to a local open source LLM for low cost dev iteration (even if the price of gpt-3.5 is super low, Iâ€™m also interested in doing all of this with a local model) Improve the retrieval : Try different chunking strategies when processing the markdown doc files Add a reranking step See you for day 3 !\n",
  "wordCount" : "940",
  "inLanguage": "en",
  "datePublished": "2024-02-09T01:23:06+01:00",
  "dateModified": "2024-02-09T01:23:06+01:00",
  "author":{
    "@type": "Person",
    "name": "Cyril LAY"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://blog.lays.pro./posts/building-a-rag-part-2/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Cyril LAY",
    "logo": {
      "@type": "ImageObject",
      "url": "https://blog.lays.pro./favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://blog.lays.pro./" accesskey="h" title="Cyril LAY (Alt + H)">Cyril LAY</a>
            <div class="logo-switches">
            </div>
        </div>
        <ul id="menu">
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Journey into building a custom RAG (Day 2)
    </h1>
    <div class="post-meta"><span title='2024-02-09 01:23:06 +0100 CET'>February 9, 2024</span>&nbsp;Â·&nbsp;Cyril LAY

</div>
  </header> 
  <div class="post-content"><p>Now that I have a basic RAG working, I performed a few experiments to assess its accuracy in retrieving relevant
documents.</p>
<p>I did an experiment where I took one part of huggingface TRL library that is supposed to be included in my embeddings,
regarding <a href="https://huggingface.co/blog/dpo-trl">DPO</a> (Direct Preference Optimization) :</p>
<p><img loading="lazy" src="/dpo-docs.png" alt="dpo docs"  />
</p>
<p>Then, I asked my model questions about it, but it wouldn&rsquo;t get them right, and outputted
something like chatGPT would, without context and knowledge about DPO.</p>
<p>At this moment, I figured out I am probably going to perform multiple experiments : adjusting the filters, parsing techniques, etc,
so I figured I will need a good tool to track the experiments, quickly retrieve the artifacts, etc.
I used code from metaflow so my pipeline was already structured as a Metaflow flow, but I didn&rsquo;t have the associated
services required to fully enjoy the features.</p>
<p>Time to deploy Metaflow service, DB and UI locally with docker so that we can easily push it to the cloud
if I need it at some point.</p>
<h3 id="deploy-metaflow">Deploy metaflow<a hidden class="anchor" aria-hidden="true" href="#deploy-metaflow">#</a></h3>
<p>To run the metaflow service (which includes one docker for metadata and one for UI backend) :</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>git clone https://github.com/Netflix/metaflow-service.git <span style="color:#f92672">&amp;&amp;</span> cd metaflow-service
</span></span><span style="display:flex;"><span>docker-compose -f docker-compose.development.yml up
</span></span></code></pre></div><p>It runs on the 8083 port by default.</p>
<p>Now run the metaflow UI front-end, using the endpoint of your just deployed service :</p>
<pre tabindex="0"><code>git clone https://github.com/Netflix/metaflow-ui/ &amp;&amp; cd metaflow-ui
docker build --tag metaflow-ui:latest .
docker run -p 3000:3000 -e METAFLOW_SERVICE=http://localhost:8083/ metaflow-ui:latest
</code></pre><p><img loading="lazy" src="/metaflow-ui.png" alt="Metaflow UI"  />
</p>
<p>It works ðŸ¥³</p>
<p>Now,I can easily visualize my runs, and if I saved some artifacts during my flow with <code>self.my_df = my_df</code> like :</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">@schedule</span>(daily<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">MarkdownChunker</span>(FlowSpec):
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">@step</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">parse_clean_docs</span>(self):
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>repo_params <span style="color:#f92672">=</span> config<span style="color:#f92672">.</span>repo_params<span style="color:#f92672">.</span>MARKDOWN_REPOS_TO_INDEX
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>parsed_filtered_df <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>pre_process_markdown()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>next(self<span style="color:#f92672">.</span>end)
</span></span></code></pre></div><p>Now, I can eventually inspect one run&rsquo;s artifacts in a notebook :</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> metaflow <span style="color:#f92672">import</span> Flow
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>flow <span style="color:#f92672">=</span> Flow(<span style="color:#e6db74">&#34;MarkdownChunker&#34;</span>)
</span></span><span style="display:flex;"><span>run <span style="color:#f92672">=</span> flow<span style="color:#f92672">.</span>latest_successful_run
</span></span><span style="display:flex;"><span>parsed_filtered_df_latest_run <span style="color:#f92672">=</span> run<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>parsed_filtered_df
</span></span></code></pre></div><p>So, <code>parsed_filtered_df_latest_run</code> is the dataframe of the parsed, clean content to be retrieved
in the RAG app. I inspected it and noticed that the DPO part I was looking for, is not in the dataframe,
so it was either filtered out, or not scraped at all, so it won&rsquo;t exist in the embedding&rsquo;s DB and can&rsquo;t be
fed as context to the LLM.</p>
<p>I now need to iterate on the parsing/filtering logic before embeddings, and figured out it was a good time to
configure a proper Vector DB, and use LLamaIndex as an abstraction of the interface between my content and the DB.</p>
<h3 id="setup-lancedb">Setup LanceDB<a hidden class="anchor" aria-hidden="true" href="#setup-lancedb">#</a></h3>
<p>I wanted to persist my embeddings so that they are not recomputed each time (it takes CPU/GPU time if done locally, money
if done with openAI&rsquo;s embedding)</p>
<p>For now it was persisted on the disk by default, but I want to switch to a proper vector DB :</p>
<ul>
<li>more efficient for querying</li>
<li>closer to a production setup</li>
</ul>
<p>I heard from my friend in ML that lanceDB was currently a great choice (open-source, very efficient (written in Rust),
easy to use), but if you need to do it yourself, I suggest comparing the available solutions yourself.
Here&rsquo;s a <a href="https://tge-data-web.nyc3.digitaloceanspaces.com/docs/Vector%20Databases%20-%20A%20Technical%20Primer.pdf">helpful pdf</a> to help you in this task.</p>
<p>Here is how to do it :</p>
<h3 id="create-the-vector-db">Create the Vector DB<a hidden class="anchor" aria-hidden="true" href="#create-the-vector-db">#</a></h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>vector_store <span style="color:#f92672">=</span> LanceDBVectorStore(uri<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;./data_lanceDB&#34;</span>) <span style="color:#75715e"># Switch to a </span>
</span></span><span style="display:flex;"><span>storage_context <span style="color:#f92672">=</span> StorageContext<span style="color:#f92672">.</span>from_defaults(vector_store<span style="color:#f92672">=</span>vector_store)
</span></span><span style="display:flex;"><span>index <span style="color:#f92672">=</span> VectorStoreIndex<span style="color:#f92672">.</span>from_vector_store(
</span></span><span style="display:flex;"><span>    vector_store, storage_context<span style="color:#f92672">=</span>storage_context
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><h3 id="embed-your-data-and-insert-it-in-the-vector-store">Embed your data and insert it in the vector store<a hidden class="anchor" aria-hidden="true" href="#embed-your-data-and-insert-it-in-the-vector-store">#</a></h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Get the latest execution of our markdown processing pipeline with metaflow</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> _run <span style="color:#f92672">in</span> Flow(<span style="color:#e6db74">&#39;DataTableProcessor&#39;</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> _run<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>save_processed_df:
</span></span><span style="display:flex;"><span>        run <span style="color:#f92672">=</span> _run
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">break</span>
</span></span><span style="display:flex;"><span>df <span style="color:#f92672">=</span> run<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>processed_df
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>vector_store <span style="color:#f92672">=</span> LanceDBVectorStore(uri<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;./data_lanceDB&#34;</span>)
</span></span><span style="display:flex;"><span>storage_context <span style="color:#f92672">=</span> StorageContext<span style="color:#f92672">.</span>from_defaults(vector_store<span style="color:#f92672">=</span>vector_store)
</span></span><span style="display:flex;"><span>index <span style="color:#f92672">=</span> VectorStoreIndex<span style="color:#f92672">.</span>from_documents(
</span></span><span style="display:flex;"><span>    [Document(t) <span style="color:#66d9ef">for</span> t <span style="color:#f92672">in</span> df<span style="color:#f92672">.</span>contents], storage_context<span style="color:#f92672">=</span>storage_context,
</span></span><span style="display:flex;"><span>    )
</span></span></code></pre></div><p>Under the hood, the last instruction actually embeds our data using the previously defined <code>service_context</code> (See
<a href="https://blog.lays.pro/posts/building-a-rag-part-1/#make-iteration-cycles-cheap">part 1</a> of this blog series), if
defined globally (<code>set_global_service_context(service_context)</code>), or you can also pass it
as a <code>service_context=service_context</code> parameter of the <code>VectorStoreIndex.from_documents().</code></p>
<h3 id="retrieve-your-existing-embeddings">Retrieve your existing embeddings<a hidden class="anchor" aria-hidden="true" href="#retrieve-your-existing-embeddings">#</a></h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">load_index</span>():
</span></span><span style="display:flex;"><span>    vector_store <span style="color:#f92672">=</span> LanceDBVectorStore(uri<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;./data_lanceDB&#34;</span>)
</span></span><span style="display:flex;"><span>    storage_context <span style="color:#f92672">=</span> StorageContext<span style="color:#f92672">.</span>from_defaults(vector_store<span style="color:#f92672">=</span>vector_store)
</span></span><span style="display:flex;"><span>    index <span style="color:#f92672">=</span> VectorStoreIndex<span style="color:#f92672">.</span>from_vector_store(
</span></span><span style="display:flex;"><span>        vector_store, storage_context<span style="color:#f92672">=</span>storage_context
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> index
</span></span></code></pre></div><p>Now that our pipeline is set to use Metaflow, we can save the index (our custom data vectorized in LanceDB)
during the flow with <code>self.embeddings_index = index</code> to access it later, at inference time for instance.</p>
<p>You can then use it to perform simple retrievals :</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>retriever <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>embeddings_index<span style="color:#f92672">.</span>as_retriever(similarity_top_k<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span>)
</span></span><span style="display:flex;"><span>query <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;How do I fine-tune a model with HuggingFace DPO ?&#34;</span>
</span></span><span style="display:flex;"><span>retrieved_nodes <span style="color:#f92672">=</span> retriever<span style="color:#f92672">.</span>retrieve(query)
</span></span><span style="display:flex;"><span>retrieved_context <span style="color:#f92672">=</span> [node<span style="color:#f92672">.</span>get_text() <span style="color:#66d9ef">for</span> node <span style="color:#f92672">in</span> retrieved_nodes]
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Retrieved context associated with query &#39;</span><span style="color:#e6db74">{</span>query<span style="color:#e6db74">}</span><span style="color:#e6db74">&#39; : </span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">{</span>retrieved_context<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span></code></pre></div><p>Or as a query engine, which will fetch the relevant prompt from our query, and embed it with our query
to ask a LLM (any LLM can be configured : openAI, local, Ollama, API, <a href="https://groq.com/">Groq</a>&hellip;) :</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> llama_index.llms.huggingface <span style="color:#f92672">import</span> HuggingFaceLLM
</span></span><span style="display:flex;"><span>llm <span style="color:#f92672">=</span> HuggingFaceLLM(model_name<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;HuggingFaceH4/zephyr-7b-alpha&#34;</span>)
</span></span><span style="display:flex;"><span>query_engine <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>embeddings_index<span style="color:#f92672">.</span>as_query_engine(service_context<span style="color:#f92672">=</span>ServiceContext<span style="color:#f92672">.</span>from_defaults(llm<span style="color:#f92672">=</span>llm))
</span></span><span style="display:flex;"><span>response <span style="color:#f92672">=</span> query_engine<span style="color:#f92672">.</span>query(query)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Response from model for query </span><span style="color:#e6db74">{</span>query<span style="color:#e6db74">}</span><span style="color:#e6db74"> : </span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">{</span>response<span style="color:#f92672">.</span>response<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span></code></pre></div><p>Or as a chat engine, which can handle context and history :</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>index<span style="color:#f92672">.</span>as_chat_engine(
</span></span><span style="display:flex;"><span>    chat_mode<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;condense_plus_context&#34;</span>
</span></span><span style="display:flex;"><span>    context_prompt<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;...Use your personal knowledge AND the context retrieved here </span><span style="color:#e6db74">{context_str}</span><span style="color:#e6db74">&#34;</span>
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><p>Various <a href="https://docs.llamaindex.ai/en/stable/module_guides/deploying/chat_engines/modules.html">chat modes</a> exist in LlamaIndex, depending
on if you want context or not, how to deal with chat history, etc.</p>
<blockquote>
<p>Note: In some usecases you&rsquo;ll want to fully disable the LLM&rsquo;s knowledge, and force it to only
use the context, to avoid hallucinations for instance.</p>
</blockquote>
<h3 id="next-steps">Next Steps<a hidden class="anchor" aria-hidden="true" href="#next-steps">#</a></h3>
<ul>
<li>Check how the default prompt logic works with LlamaIndex</li>
<li>Verify that the model can use both of its existing knowledge, and our embeddings'</li>
<li>Switch OpenAI&rsquo;s gpt to a local open source LLM for low cost dev iteration (even if the price of gpt-3.5 is super low,
I&rsquo;m also interested in doing all of this with a local model)</li>
<li>Improve the retrieval :
<ul>
<li>Try different chunking strategies when processing the markdown doc files</li>
<li>Add a <a href="https://blog.llamaindex.ai/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83">reranking</a>
step</li>
</ul>
</li>
</ul>
<p>See you for day 3 !</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2024 <a href="https://blog.lays.pro./">Cyril LAY</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
