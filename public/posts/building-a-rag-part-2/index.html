<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Journey into building a custom RAG (Day 2) | Cyril LAY</title>
<meta name="keywords" content="">
<meta name="description" content="Now, I want to check if the context is well retrieved. I did an experiment where I took one part of huggingface TRL library that is supposed to be included in my embeddings, regarding DPO:
I asked my model questions about DPO, but it wouldn&rsquo;t get them right, it seems like he didn&rsquo;t get it in its context.
Thanks to my metaflow pipeline I could access the dataframe containing the data before embedding and indexing (Flow.">
<meta name="author" content="Cyril LAY">
<link rel="canonical" href="https://blog.lays.pro./posts/building-a-rag-part-2/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.5ff2630c4d1b3e25bc21f0ecd96681dbcf58219e741fa627857820b5485cb770.css" integrity="sha256-X/JjDE0bPiW8IfDs2WaB289YIZ50H6YnhXggtUhct3A=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://blog.lays.pro./favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://blog.lays.pro./favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://blog.lays.pro./favicon-32x32.png">
<link rel="apple-touch-icon" href="https://blog.lays.pro./apple-touch-icon.png">
<link rel="mask-icon" href="https://blog.lays.pro./safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript><meta property="og:title" content="Journey into building a custom RAG (Day 2)" />
<meta property="og:description" content="Now, I want to check if the context is well retrieved. I did an experiment where I took one part of huggingface TRL library that is supposed to be included in my embeddings, regarding DPO:
I asked my model questions about DPO, but it wouldn&rsquo;t get them right, it seems like he didn&rsquo;t get it in its context.
Thanks to my metaflow pipeline I could access the dataframe containing the data before embedding and indexing (Flow." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://blog.lays.pro./posts/building-a-rag-part-2/" /><meta property="og:image" content="https://blog.lays.pro./assets/images/mixtral.png"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-02-09T01:23:06+01:00" />
<meta property="article:modified_time" content="2024-02-09T01:23:06+01:00" /><meta property="og:site_name" content="AI &amp; Data blog by Cyril LAY" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://blog.lays.pro./assets/images/mixtral.png"/>

<meta name="twitter:title" content="Journey into building a custom RAG (Day 2)"/>
<meta name="twitter:description" content="Now, I want to check if the context is well retrieved. I did an experiment where I took one part of huggingface TRL library that is supposed to be included in my embeddings, regarding DPO:
I asked my model questions about DPO, but it wouldn&rsquo;t get them right, it seems like he didn&rsquo;t get it in its context.
Thanks to my metaflow pipeline I could access the dataframe containing the data before embedding and indexing (Flow."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://blog.lays.pro./posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Journey into building a custom RAG (Day 2)",
      "item": "https://blog.lays.pro./posts/building-a-rag-part-2/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Journey into building a custom RAG (Day 2)",
  "name": "Journey into building a custom RAG (Day 2)",
  "description": "Now, I want to check if the context is well retrieved. I did an experiment where I took one part of huggingface TRL library that is supposed to be included in my embeddings, regarding DPO:\nI asked my model questions about DPO, but it wouldn\u0026rsquo;t get them right, it seems like he didn\u0026rsquo;t get it in its context.\nThanks to my metaflow pipeline I could access the dataframe containing the data before embedding and indexing (Flow.",
  "keywords": [
    
  ],
  "articleBody": "Now, I want to check if the context is well retrieved. I did an experiment where I took one part of huggingface TRL library that is supposed to be included in my embeddings, regarding DPO:\nI asked my model questions about DPO, but it wouldnâ€™t get them right, it seems like he didnâ€™t get it in its context.\nThanks to my metaflow pipeline I could access the dataframe containing the data before embedding and indexing (Flow.run.df), and check whatâ€™s going on : itâ€™s not in the dataframe, so it was either filtered out, or not scraped at all.\nAt this moment, I figured out I am probably going to perform multiple experiments : adjusting the filters, parsing techniques, etc, so I figured I will need a good tool to track the experiments, quickly retrieve the artifacts, etc. Time to deploy metaflow service, DB and UI locally, but with docker so that we can easily push it to the cloud if I need it at some point.\nTo run the metaflow service (which includes one docker for metadata and one for UI backend) :\ngit clone https://github.com/Netflix/metaflow-service.git \u0026\u0026 cd metaflow-service docker-compose -f docker-compose.development.yml up It runs on the 8083 port by default.\nNow run the metaflow UI front-end, using the endpoint of your just deployed service :\ngit clone https://github.com/Netflix/metaflow-ui/ \u0026\u0026 cd metaflow-ui docker build --tag metaflow-ui:latest . docker run -p 3000:3000 -e METAFLOW_SERVICE=http://localhost:8083/ metaflow-ui:latest It works ðŸ¥³\nSwitch to lanceDB I wanted to persist my embeddings so that they are not recomputed each time (it takes CPU/GPU time if done locally, money if done with openAIâ€™s embedding) For now it was persisted on the disk by default, but I want to switch to a proper vector DB :\nmore efficient for querying closer to a production setup I heard from my friend in ML that lanceDB was currently a great choice (open-source, very efficient (written in Rust), easy to use), but if you need to do it yourself, I suggest comparing the available solutions yourself. Hereâ€™s a helpful pdf to help you in this task.\nHere is how to do it :\nCreate the Vector DB vector_store = LanceDBVectorStore(uri=\"./data_lanceDB\") # Switch to a storage_context = StorageContext.from_defaults(vector_store=vector_store) index = VectorStoreIndex.from_vector_store( vector_store, storage_context=storage_context ) Embed your data and insert it in the vector store # Get the latest execution of our markdown processing pipeline with metaflow for _run in Flow('DataTableProcessor'): if _run.data.save_processed_df: run = _run break df = run.data.processed_df vector_store = LanceDBVectorStore(uri=\"./data_lanceDB\") storage_context = StorageContext.from_defaults(vector_store=vector_store) index = VectorStoreIndex.from_documents( [Document(t) for t in df.contents], storage_context=storage_context, ) Under the hood, the last instruction actually embeds our data using the previously defined service_context (See part 1 of this blog series), if defined globally (set_global_service_context(service_context)), or you can also pass it as a service_context=service_context parameter of the VectorStoreIndex.from_documents().\nRetrieve your existing embeddings def load_index(): vector_store = LanceDBVectorStore(uri=\"./data_lanceDB\") storage_context = StorageContext.from_defaults(vector_store=vector_store) index = VectorStoreIndex.from_vector_store( vector_store, storage_context=storage_context ) return index Next Steps Check how the default prompt logic works with LlamaIndex Verify that the model can use both of itâ€™s existing knowledge, and the onf from our embeddings Switch OpenAIâ€™s gpt to a local open source LLM for low cost dev iteration (even if the price of gpt-3.5 is super low, Iâ€™m also interested in doing all of this with a local model) See you for day 3 !\n",
  "wordCount" : "551",
  "inLanguage": "en",
  "datePublished": "2024-02-09T01:23:06+01:00",
  "dateModified": "2024-02-09T01:23:06+01:00",
  "author":{
    "@type": "Person",
    "name": "Cyril LAY"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://blog.lays.pro./posts/building-a-rag-part-2/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Cyril LAY",
    "logo": {
      "@type": "ImageObject",
      "url": "https://blog.lays.pro./favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://blog.lays.pro./" accesskey="h" title="Cyril LAY (Alt + H)">Cyril LAY</a>
            <div class="logo-switches">
            </div>
        </div>
        <ul id="menu">
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Journey into building a custom RAG (Day 2)
    </h1>
    <div class="post-meta"><span title='2024-02-09 01:23:06 +0100 CET'>February 9, 2024</span>&nbsp;Â·&nbsp;Cyril LAY

</div>
  </header> 
  <div class="post-content"><p>Now, I want to check if the context is well retrieved.
I did an experiment where I took one part of huggingface TRL library that is supposed to be included in my embeddings,
regarding DPO:</p>
<p><img loading="lazy" src="/dpo-docs.png" alt="dpo docs"  />
</p>
<p>I asked my model questions about DPO, but it wouldn&rsquo;t get them right, it seems like he didn&rsquo;t get it in its context.</p>
<p>Thanks to my metaflow pipeline I could access the dataframe containing the data before embedding
and indexing (<code>Flow.run.df</code>), and check what&rsquo;s going on : it&rsquo;s not in the dataframe, so it was either
filtered out, or not scraped at all.</p>
<p>At this moment, I figured out I am probably going to perform multiple experiments : adjusting the filters, parsing techniques, etc,
so I figured I will need a good tool to track the experiments, quickly retrieve the artifacts, etc.
Time to deploy metaflow service, DB and UI locally, but with docker so that we can easily push it to the cloud
if I need it at some point.</p>
<p>To run the metaflow service (which includes one docker for metadata and one for UI backend) :</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>git clone https://github.com/Netflix/metaflow-service.git <span style="color:#f92672">&amp;&amp;</span> cd metaflow-service
</span></span><span style="display:flex;"><span>docker-compose -f docker-compose.development.yml up
</span></span></code></pre></div><p>It runs on the 8083 port by default.</p>
<p>Now run the metaflow UI front-end, using the endpoint of your just deployed service :</p>
<pre tabindex="0"><code>git clone https://github.com/Netflix/metaflow-ui/ &amp;&amp; cd metaflow-ui
docker build --tag metaflow-ui:latest .
docker run -p 3000:3000 -e METAFLOW_SERVICE=http://localhost:8083/ metaflow-ui:latest
</code></pre><p><img loading="lazy" src="/metaflow-ui.png" alt="Metaflow UI"  />
</p>
<p>It works ðŸ¥³</p>
<h3 id="switch-to-lancedb">Switch to lanceDB<a hidden class="anchor" aria-hidden="true" href="#switch-to-lancedb">#</a></h3>
<p>I wanted to persist my embeddings so that they are not recomputed each time (it takes CPU/GPU time if done locally, money
if done with openAI&rsquo;s embedding)
For now it was persisted on the disk by default, but I want to switch to a proper vector DB :</p>
<ul>
<li>more efficient for querying</li>
<li>closer to a production setup</li>
</ul>
<p>I heard from my friend in ML that lanceDB was currently a great choice (open-source, very efficient (written in Rust),
easy to use), but if you need to do it yourself, I suggest comparing the available solutions yourself.
Here&rsquo;s a <a href="https://tge-data-web.nyc3.digitaloceanspaces.com/docs/Vector%20Databases%20-%20A%20Technical%20Primer.pdf">helpful pdf</a> to help you in this task.</p>
<p>Here is how to do it :</p>
<h3 id="create-the-vector-db">Create the Vector DB<a hidden class="anchor" aria-hidden="true" href="#create-the-vector-db">#</a></h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>vector_store <span style="color:#f92672">=</span> LanceDBVectorStore(uri<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;./data_lanceDB&#34;</span>) <span style="color:#75715e"># Switch to a </span>
</span></span><span style="display:flex;"><span>storage_context <span style="color:#f92672">=</span> StorageContext<span style="color:#f92672">.</span>from_defaults(vector_store<span style="color:#f92672">=</span>vector_store)
</span></span><span style="display:flex;"><span>index <span style="color:#f92672">=</span> VectorStoreIndex<span style="color:#f92672">.</span>from_vector_store(
</span></span><span style="display:flex;"><span>    vector_store, storage_context<span style="color:#f92672">=</span>storage_context
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><h3 id="embed-your-data-and-insert-it-in-the-vector-store">Embed your data and insert it in the vector store<a hidden class="anchor" aria-hidden="true" href="#embed-your-data-and-insert-it-in-the-vector-store">#</a></h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Get the latest execution of our markdown processing pipeline with metaflow</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> _run <span style="color:#f92672">in</span> Flow(<span style="color:#e6db74">&#39;DataTableProcessor&#39;</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> _run<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>save_processed_df:
</span></span><span style="display:flex;"><span>        run <span style="color:#f92672">=</span> _run
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">break</span>
</span></span><span style="display:flex;"><span>df <span style="color:#f92672">=</span> run<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>processed_df
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>vector_store <span style="color:#f92672">=</span> LanceDBVectorStore(uri<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;./data_lanceDB&#34;</span>)
</span></span><span style="display:flex;"><span>storage_context <span style="color:#f92672">=</span> StorageContext<span style="color:#f92672">.</span>from_defaults(vector_store<span style="color:#f92672">=</span>vector_store)
</span></span><span style="display:flex;"><span>index <span style="color:#f92672">=</span> VectorStoreIndex<span style="color:#f92672">.</span>from_documents(
</span></span><span style="display:flex;"><span>    [Document(t) <span style="color:#66d9ef">for</span> t <span style="color:#f92672">in</span> df<span style="color:#f92672">.</span>contents], storage_context<span style="color:#f92672">=</span>storage_context,
</span></span><span style="display:flex;"><span>    )
</span></span></code></pre></div><p>Under the hood, the last instruction actually embeds our data using the previously defined <code>service_context</code> (See
<a href="https://blog.lays.pro/posts/building-a-rag-part-1/#make-iteration-cycles-cheap">part 1</a> of this blog series), if
defined globally (<code>set_global_service_context(service_context)</code>), or you can also pass it
as a <code>service_context=service_context</code> parameter of the <code>VectorStoreIndex.from_documents().</code></p>
<h3 id="retrieve-your-existing-embeddings">Retrieve your existing embeddings<a hidden class="anchor" aria-hidden="true" href="#retrieve-your-existing-embeddings">#</a></h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">load_index</span>():
</span></span><span style="display:flex;"><span>    vector_store <span style="color:#f92672">=</span> LanceDBVectorStore(uri<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;./data_lanceDB&#34;</span>)
</span></span><span style="display:flex;"><span>    storage_context <span style="color:#f92672">=</span> StorageContext<span style="color:#f92672">.</span>from_defaults(vector_store<span style="color:#f92672">=</span>vector_store)
</span></span><span style="display:flex;"><span>    index <span style="color:#f92672">=</span> VectorStoreIndex<span style="color:#f92672">.</span>from_vector_store(
</span></span><span style="display:flex;"><span>        vector_store, storage_context<span style="color:#f92672">=</span>storage_context
</span></span><span style="display:flex;"><span>    )
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> index
</span></span></code></pre></div><h3 id="next-steps">Next Steps<a hidden class="anchor" aria-hidden="true" href="#next-steps">#</a></h3>
<ul>
<li>Check how the default prompt logic works with LlamaIndex</li>
<li>Verify that the model can use both of it&rsquo;s existing knowledge,
and the onf from our embeddings</li>
<li>Switch OpenAI&rsquo;s gpt to a local open source LLM for low cost dev iteration (even if the price of gpt-3.5 is super low,
I&rsquo;m also interested in doing all of this with a local model)</li>
</ul>
<p>See you for day 3 !</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2024 <a href="https://blog.lays.pro./">Cyril LAY</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
