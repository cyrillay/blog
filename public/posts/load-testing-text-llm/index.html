<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Load Testing Text Llm | Cyril LAY</title>
<meta name="keywords" content="">
<meta name="description" content="Load Testing Open-Source LLMs on H100 I am working as an LLMOps engineer since a few years, currently on a mission for the French government.
Our team is building an API gateway to provide French administrations with access to open-source large language models (LLMs).
I recently focused on load testing newer models to evaluate if they could become candidates for production.
This post shares results and methodology for text models.">
<meta name="author" content="Cyril LAY">
<link rel="canonical" href="https://blog.lays.pro./posts/load-testing-text-llm/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.5ff2630c4d1b3e25bc21f0ecd96681dbcf58219e741fa627857820b5485cb770.css" integrity="sha256-X/JjDE0bPiW8IfDs2WaB289YIZ50H6YnhXggtUhct3A=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://blog.lays.pro./favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://blog.lays.pro./favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://blog.lays.pro./favicon-32x32.png">
<link rel="apple-touch-icon" href="https://blog.lays.pro./apple-touch-icon.png">
<link rel="mask-icon" href="https://blog.lays.pro./safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript><meta property="og:title" content="Load Testing Text Llm" />
<meta property="og:description" content="Load Testing Open-Source LLMs on H100 I am working as an LLMOps engineer since a few years, currently on a mission for the French government.
Our team is building an API gateway to provide French administrations with access to open-source large language models (LLMs).
I recently focused on load testing newer models to evaluate if they could become candidates for production.
This post shares results and methodology for text models." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://blog.lays.pro./posts/load-testing-text-llm/" /><meta property="og:image" content="https://blog.lays.pro./assets/images/mixtral.png"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2025-09-30T15:17:57+02:00" />
<meta property="article:modified_time" content="2025-09-30T15:17:57+02:00" /><meta property="og:site_name" content="AI &amp; Data blog by Cyril LAY" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://blog.lays.pro./assets/images/mixtral.png"/>

<meta name="twitter:title" content="Load Testing Text Llm"/>
<meta name="twitter:description" content="Load Testing Open-Source LLMs on H100 I am working as an LLMOps engineer since a few years, currently on a mission for the French government.
Our team is building an API gateway to provide French administrations with access to open-source large language models (LLMs).
I recently focused on load testing newer models to evaluate if they could become candidates for production.
This post shares results and methodology for text models."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://blog.lays.pro./posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Load Testing Text Llm",
      "item": "https://blog.lays.pro./posts/load-testing-text-llm/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Load Testing Text Llm",
  "name": "Load Testing Text Llm",
  "description": "Load Testing Open-Source LLMs on H100 I am working as an LLMOps engineer since a few years, currently on a mission for the French government.\nOur team is building an API gateway to provide French administrations with access to open-source large language models (LLMs).\nI recently focused on load testing newer models to evaluate if they could become candidates for production.\nThis post shares results and methodology for text models.",
  "keywords": [
    
  ],
  "articleBody": "Load Testing Open-Source LLMs on H100 I am working as an LLMOps engineer since a few years, currently on a mission for the French government.\nOur team is building an API gateway to provide French administrations with access to open-source large language models (LLMs).\nI recently focused on load testing newer models to evaluate if they could become candidates for production.\nThis post shares results and methodology for text models.\n(Next week I‚Äôll publish the same analysis for audio models.)\nBenchmark Setup All benchmarks were run with the following setup:\nHardware: 1√ó NVIDIA H100 (80 GB VRAM), 24 CPU cores, 230 GB RAM Framework: vLLM 0.10.2 Environment: Kubernetes cluster (cloud deployment) Dataset: prompts sampled randomly from Alpaca dataset Prefix caching: disabled (to avoid biased results) Protocol: For each concurrency level, run 3 iterations Results averaged to reduce variability from prompt length and output size Throughput vs. Latency üìä [placeholder: graph TTFT comparison across models]\nüìä [placeholder: graph throughput comparison across models]\nA Note on ‚ÄúConcurrent Requests‚Äù vs. ‚ÄúUsers‚Äù In practice, the number of concurrent requests is not equal to the number of simultaneous users.\nUsers do not all send queries at the exact same time.\nExample:\n100 users Each sends a request every ~40 s Average request takes 20 s to process On average, each user is ‚Äúactive‚Äù (waiting for a response) 20/40 = 50% of the time.\nSo 100 users ‚âà 50 concurrent requests.\nThis estimation is rough and depends on the type of usage:\nHuman interacting with a chatbot ‚Üí lower concurrency Automated classification pipeline ‚Üí higher concurrency Key Takeaways Benchmarks were run on H100 (80 GB VRAM) with vLLM 0.10.2 in a Kubernetes environment TTFT and throughput vary significantly depending on the model, even on identical hardware Concurrency levels in benchmarks are not directly equal to number of end-users Results provide a baseline for evaluating candidate models for production usage Next Steps I will share benchmarks for audio models next week.\nIn the meantime:\nHave you measured similar metrics? Do your results align, or differ, on other hardware / deployment setups? üëâ If you are working on LLM deployments and want to exchange notes on benchmarking methodology or results, feel free to reach out.\n",
  "wordCount" : "367",
  "inLanguage": "en",
  "datePublished": "2025-09-30T15:17:57+02:00",
  "dateModified": "2025-09-30T15:17:57+02:00",
  "author":{
    "@type": "Person",
    "name": "Cyril LAY"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://blog.lays.pro./posts/load-testing-text-llm/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Cyril LAY",
    "logo": {
      "@type": "ImageObject",
      "url": "https://blog.lays.pro./favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://blog.lays.pro./" accesskey="h" title="Cyril LAY (Alt + H)">Cyril LAY</a>
            <div class="logo-switches">
            </div>
        </div>
        <ul id="menu">
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Load Testing Text Llm
    </h1>
    <div class="post-meta"><span title='2025-09-30 15:17:57 +0200 CEST'>September 30, 2025</span>&nbsp;¬∑&nbsp;Cyril LAY

</div>
  </header> 
  <div class="post-content"><h1 id="load-testing-open-source-llms-on-h100">Load Testing Open-Source LLMs on H100<a hidden class="anchor" aria-hidden="true" href="#load-testing-open-source-llms-on-h100">#</a></h1>
<p>I am working as an LLMOps engineer since a few years, currently on a mission for the French government.<br>
Our team is building an API gateway to provide French administrations with access to open-source large language models (LLMs).<br>
I recently focused on <strong>load testing newer models</strong> to evaluate if they could become candidates for production.</p>
<p>This post shares results and methodology for <strong>text models</strong>.<br>
(Next week I‚Äôll publish the same analysis for <strong>audio models</strong>.)</p>
<hr>
<h2 id="benchmark-setup">Benchmark Setup<a hidden class="anchor" aria-hidden="true" href="#benchmark-setup">#</a></h2>
<p>All benchmarks were run with the following setup:</p>
<ul>
<li><strong>Hardware:</strong> 1√ó NVIDIA H100 (80 GB VRAM), 24 CPU cores, 230 GB RAM</li>
<li><strong>Framework:</strong> <a href="https://github.com/vllm-project/vllm">vLLM 0.10.2</a></li>
<li><strong>Environment:</strong> Kubernetes cluster (cloud deployment)</li>
<li><strong>Dataset:</strong> prompts sampled randomly from <a href="https://github.com/tatsu-lab/stanford_alpaca">Alpaca dataset</a></li>
<li><strong>Prefix caching:</strong> disabled (to avoid biased results)</li>
<li><strong>Protocol:</strong>
<ul>
<li>For each concurrency level, run 3 iterations</li>
<li>Results averaged to reduce variability from prompt length and output size</li>
</ul>
</li>
</ul>
<hr>
<h2 id="throughput-vs-latency">Throughput vs. Latency<a hidden class="anchor" aria-hidden="true" href="#throughput-vs-latency">#</a></h2>
<p>üìä <em>[placeholder: graph TTFT comparison across models]</em><br>
üìä <em>[placeholder: graph throughput comparison across models]</em></p>
<hr>
<h2 id="a-note-on-concurrent-requests-vs-users">A Note on ‚ÄúConcurrent Requests‚Äù vs. ‚ÄúUsers‚Äù<a hidden class="anchor" aria-hidden="true" href="#a-note-on-concurrent-requests-vs-users">#</a></h2>
<p>In practice, the number of concurrent requests is not equal to the number of simultaneous users.<br>
Users do not all send queries at the exact same time.</p>
<p>Example:</p>
<ul>
<li>100 users</li>
<li>Each sends a request every ~40 s</li>
<li>Average request takes 20 s to process</li>
</ul>
<p>On average, each user is ‚Äúactive‚Äù (waiting for a response) 20/40 = 50% of the time.<br>
So 100 users ‚âà <strong>50 concurrent requests</strong>.</p>
<p>This estimation is rough and depends on the type of usage:</p>
<ul>
<li>Human interacting with a chatbot ‚Üí lower concurrency</li>
<li>Automated classification pipeline ‚Üí higher concurrency</li>
</ul>
<hr>
<h2 id="key-takeaways">Key Takeaways<a hidden class="anchor" aria-hidden="true" href="#key-takeaways">#</a></h2>
<ul>
<li>Benchmarks were run on <strong>H100 (80 GB VRAM)</strong> with <strong>vLLM 0.10.2</strong> in a Kubernetes environment</li>
<li><strong>TTFT and throughput vary significantly</strong> depending on the model, even on identical hardware</li>
<li>Concurrency levels in benchmarks are <strong>not directly equal</strong> to number of end-users</li>
<li>Results provide a baseline for <strong>evaluating candidate models for production usage</strong></li>
</ul>
<hr>
<h2 id="next-steps">Next Steps<a hidden class="anchor" aria-hidden="true" href="#next-steps">#</a></h2>
<p>I will share benchmarks for <strong>audio models</strong> next week.</p>
<p>In the meantime:</p>
<ul>
<li>Have you measured similar metrics?</li>
<li>Do your results align, or differ, on other hardware / deployment setups?</li>
</ul>
<hr>
<p>üëâ <em>If you are working on LLM deployments and want to exchange notes on benchmarking methodology or results, feel free to reach out.</em></p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2025 <a href="https://blog.lays.pro./">Cyril LAY</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
