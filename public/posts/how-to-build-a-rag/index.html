<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Develop a Retrieval Augmented Generation (RAG) LLM Application | Cyril LAY</title>
<meta name="keywords" content="">
<meta name="description" content="This article explains how to customize a large language model for your specific needs.
There are many approaches to adapt a Large Language Model (LLM) for your data, with the goal of enhancing the responses relevance.
Prompt engineering : you spend time working on a good prompt, eventually a templated prompt where you can include your data. This is the fastest way, but will also quickly become limited, as the context size (maximum input you can give to a model) is usually in the range of 4 - 16K tokens (3 - 11K words), and it is shared between question and response.">
<meta name="author" content="Cyril LAY">
<link rel="canonical" href="https://blog.lays.pro./posts/how-to-build-a-rag/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.5ff2630c4d1b3e25bc21f0ecd96681dbcf58219e741fa627857820b5485cb770.css" integrity="sha256-X/JjDE0bPiW8IfDs2WaB289YIZ50H6YnhXggtUhct3A=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://blog.lays.pro./favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://blog.lays.pro./favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://blog.lays.pro./favicon-32x32.png">
<link rel="apple-touch-icon" href="https://blog.lays.pro./apple-touch-icon.png">
<link rel="mask-icon" href="https://blog.lays.pro./safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript><meta property="og:title" content="Develop a Retrieval Augmented Generation (RAG) LLM Application" />
<meta property="og:description" content="This article explains how to customize a large language model for your specific needs.
There are many approaches to adapt a Large Language Model (LLM) for your data, with the goal of enhancing the responses relevance.
Prompt engineering : you spend time working on a good prompt, eventually a templated prompt where you can include your data. This is the fastest way, but will also quickly become limited, as the context size (maximum input you can give to a model) is usually in the range of 4 - 16K tokens (3 - 11K words), and it is shared between question and response." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://blog.lays.pro./posts/how-to-build-a-rag/" /><meta property="og:image" content="https://blog.lays.pro./assets/images/mixtral.png"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-02-06T23:09:33+01:00" />
<meta property="article:modified_time" content="2024-02-06T23:09:33+01:00" /><meta property="og:site_name" content="AI &amp; Data blog by Cyril LAY" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://blog.lays.pro./assets/images/mixtral.png"/>

<meta name="twitter:title" content="Develop a Retrieval Augmented Generation (RAG) LLM Application"/>
<meta name="twitter:description" content="This article explains how to customize a large language model for your specific needs.
There are many approaches to adapt a Large Language Model (LLM) for your data, with the goal of enhancing the responses relevance.
Prompt engineering : you spend time working on a good prompt, eventually a templated prompt where you can include your data. This is the fastest way, but will also quickly become limited, as the context size (maximum input you can give to a model) is usually in the range of 4 - 16K tokens (3 - 11K words), and it is shared between question and response."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://blog.lays.pro./posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Develop a Retrieval Augmented Generation (RAG) LLM Application",
      "item": "https://blog.lays.pro./posts/how-to-build-a-rag/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Develop a Retrieval Augmented Generation (RAG) LLM Application",
  "name": "Develop a Retrieval Augmented Generation (RAG) LLM Application",
  "description": "This article explains how to customize a large language model for your specific needs.\nThere are many approaches to adapt a Large Language Model (LLM) for your data, with the goal of enhancing the responses relevance.\nPrompt engineering : you spend time working on a good prompt, eventually a templated prompt where you can include your data. This is the fastest way, but will also quickly become limited, as the context size (maximum input you can give to a model) is usually in the range of 4 - 16K tokens (3 - 11K words), and it is shared between question and response.",
  "keywords": [
    
  ],
  "articleBody": "This article explains how to customize a large language model for your specific needs.\nThere are many approaches to adapt a Large Language Model (LLM) for your data, with the goal of enhancing the responses relevance.\nPrompt engineering : you spend time working on a good prompt, eventually a templated prompt where you can include your data. This is the fastest way, but will also quickly become limited, as the context size (maximum input you can give to a model) is usually in the range of 4 - 16K tokens (3 - 11K words), and it is shared between question and response. So you won‚Äôt be able to include all of your data in the prompt (or you will do so at the expense of the response length)\nFine-tuning : you can take existing models and fine-tune them with your own data. You could use openAI‚Äôs fine-tuning API (can become expensive if you don‚Äôt get it right the first time), or fine-tune an open-source model, like mistral, mixtral, llama2. This approach yields great results, but is still expensive in terms of engineering resources and computing costs, as one needs to go through model training iteration loops, harder than iterating over a prompt. More information on when to fine-tune here.\nTraining from scratch : this is so prohibitively expensive that only companies like OpenAI, Meta, Google can afford the 1000s of GPUs required to perform what we call the pretraining (understand basic language and concepts by learning to predict the next token)\nRetrieval Augmented generation (RAG) : it is a good trade off with medium engineering complexity and costs, and high impact on the accuracy and relevance of the augmented model. In a way, it‚Äôs a bit like prompt engineering on steroids. Let‚Äôs deep dive into it and see how to make one.\nRAG is a technique for enhancing the accuracy, reliability and relevance of generative AI models by giving it ultra relevant facts at query time, previously fetched from internal or external sources and vectorized as embeddings.\nHere is how such a system can be built :\nLet‚Äôs dive into building a RAG-based LLM application using publicly available data.\nWhat we‚Äôll cover: üíª Setting up and processing workloads locally. üöÄ Scaling your solution to the cloud effortlessly. üí° Enhancing performance with reranking, fine-tuning, and prompt engineering. Steps 1. Get the data Start by gathering data from public sources. For this project, we aim for a structure like this:\n| url | section | text | | --- | ------- | ---- | | https://example.com | Introduction | \"Here's how to start with LLM...\" | | https://example.com | Getting Data | \"Data is crucial for training LLMs...\" | | https://example.com | Training | \"To train your model, begin with...\" | Future articles will delve into using internal/proprietary databases.\n2. Preprocess the data Normalize data by chunking larger sections and combining smaller ones. Here‚Äôs a simple Python snippet to chunk text data:\nfrom textwrap import wrap def chunk_text(text, chunk_size=512): return wrap(text, chunk_size) # Example usage chunked_section = chunk_text(long_text_section) 3. Transform text data to embeddings Embeddings turn text into a numerical format that‚Äôs understandable by LLMs. We‚Äôll use the Instructorüë®‚Äçüè´ model for its performance:\nfrom sentence_transformers import SentenceTransformer model = SentenceTransformer('hkunlp/instructor-large') def embed_text(text): return model.encode(text) # Example usage embedded_text = embed_text(\"Your text here\") Note: While we start with embeddings for efficiency, exploring LLMs for the top k chunks and reranking with tools like Cohere Rerank could further refine relevance.\n4. Store the data in a vectorDB We‚Äôll use pgvector for storage. Here‚Äôs how to set it up with PostgreSQL:\nCREATE EXTENSION pgvector; CREATE TABLE documents (id SERIAL PRIMARY KEY, text_vector vector(512)); And inserting data:\nimport psycopg2 # Connect to your PostgreSQL database conn = psycopg2.connect(\"dbname=your_db user=your_user\") cur = conn.cursor() # Insert the embedded text cur.execute(\"INSERT INTO documents (text_vector) VALUES (vector(%s))\", (embedded_text,)) conn.commit() 5. Perform inference For inference, we retrieve relevant documents, optionally rerank them, and compile a contextual prompt:\ndef query_vector_db(query_embedding, top_k=5): # Query the top_k most similar vectors cur.execute(\"SELECT id, text_vector \u003c-\u003e vector(%s) AS distance FROM documents ORDER BY distance ASC LIMIT %s\", (query_embedding, top_k)) return cur.fetchall() # Example: embedding a query and retrieving related documents query_embedding = embed_text(\"Your query here\") related_documents = query_vector_db(query_embedding) Conclusion Building a RAG-based LLM from scratch teaches you the nuts and bolts of modern NLP applications, from data preprocessing to deploying scalable solutions. This guide is just the beginning; stay tuned for more in-depth exploration of fine-tuning and prompt engineering techniques.\n",
  "wordCount" : "744",
  "inLanguage": "en",
  "datePublished": "2024-02-06T23:09:33+01:00",
  "dateModified": "2024-02-06T23:09:33+01:00",
  "author":{
    "@type": "Person",
    "name": "Cyril LAY"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://blog.lays.pro./posts/how-to-build-a-rag/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Cyril LAY",
    "logo": {
      "@type": "ImageObject",
      "url": "https://blog.lays.pro./favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://blog.lays.pro./" accesskey="h" title="Cyril LAY (Alt + H)">Cyril LAY</a>
            <div class="logo-switches">
            </div>
        </div>
        <ul id="menu">
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Develop a Retrieval Augmented Generation (RAG) LLM Application
    </h1>
    <div class="post-meta"><span title='2024-02-06 23:09:33 +0100 CET'>February 6, 2024</span>&nbsp;¬∑&nbsp;Cyril LAY

</div>
  </header> 
  <div class="post-content"><p>This article explains how to customize a large language model for your specific needs.</p>
<p>There are many approaches to adapt a Large Language Model (LLM) for your data, with the goal of enhancing the responses relevance.</p>
<ol>
<li>
<p>Prompt engineering : you spend time working on a good prompt, eventually a templated prompt where you
can include your data. This is the fastest way, but will also quickly become limited, as the context size
(maximum input you can give to a model) is usually in the range of 4 - 16K tokens (3 - 11K words), and it is shared between question
and response. So you won&rsquo;t be able to include all of your data in the prompt (or you will do so at the expense of the response length)</p>
</li>
<li>
<p>Fine-tuning : you can take existing models and fine-tune them with your own data. You could use openAI&rsquo;s
fine-tuning API (can become expensive if you don&rsquo;t get it right the first time), or fine-tune an open-source model,
like mistral, mixtral, llama2. This approach yields great results, but is still expensive in terms of engineering
resources and computing costs, as one needs to go through model training iteration loops, harder than iterating over a prompt.
More information on when to fine-tune <a href="https://platform.openai.com/docs/guides/fine-tuning/when-to-use-fine-tuning">here</a>.</p>
</li>
<li>
<p>Training from scratch : this is so prohibitively expensive that only companies like OpenAI, Meta, Google can afford
the 1000s of GPUs required to perform what we call the <strong>pretraining</strong> (understand basic language and concepts by learning to predict the next token)</p>
</li>
<li>
<p>Retrieval Augmented generation (RAG) : it is a good trade off with medium engineering complexity and costs,
and high impact on the accuracy and relevance of the augmented model. In a way, it&rsquo;s a bit like prompt engineering on steroids.
Let&rsquo;s deep dive into it and see how to make one.</p>
</li>
</ol>
<p>RAG is a technique for enhancing the accuracy, reliability and relevance of generative AI models by giving
it ultra relevant facts at query time, previously fetched from internal or external sources and vectorized as embeddings.</p>
<p>Here is how such a system can be built :</p>
<p><img loading="lazy" src="/rag-archi.png" alt="RAG architecture"  />
</p>
<p>Let&rsquo;s dive into building a RAG-based LLM application using publicly available data.</p>
<h3 id="what-well-cover">What we&rsquo;ll cover:<a hidden class="anchor" aria-hidden="true" href="#what-well-cover">#</a></h3>
<ul>
<li>üíª Setting up and processing workloads locally.</li>
<li>üöÄ Scaling your solution to the cloud effortlessly.</li>
<li>üí° Enhancing performance with reranking, fine-tuning, and prompt engineering.</li>
</ul>
<h3 id="steps">Steps<a hidden class="anchor" aria-hidden="true" href="#steps">#</a></h3>
<h4 id="1-get-the-data">1. Get the data<a hidden class="anchor" aria-hidden="true" href="#1-get-the-data">#</a></h4>
<p>Start by gathering data from public sources. For this project, we aim for a structure like this:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-markdown" data-lang="markdown"><span style="display:flex;"><span>| url | section | text |
</span></span><span style="display:flex;"><span>| --- | ------- | ---- |
</span></span><span style="display:flex;"><span>| https://example.com | Introduction | &#34;Here&#39;s how to start with LLM...&#34; |
</span></span><span style="display:flex;"><span>| https://example.com | Getting Data | &#34;Data is crucial for training LLMs...&#34; |
</span></span><span style="display:flex;"><span>| https://example.com | Training | &#34;To train your model, begin with...&#34; |
</span></span></code></pre></div><p>Future articles will delve into using internal/proprietary databases.</p>
<h4 id="2-preprocess-the-data">2. Preprocess the data<a hidden class="anchor" aria-hidden="true" href="#2-preprocess-the-data">#</a></h4>
<p>Normalize data by chunking larger sections and combining smaller ones. Here‚Äôs a simple Python snippet to chunk text data:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> textwrap <span style="color:#f92672">import</span> wrap
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">chunk_text</span>(text, chunk_size<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> wrap(text, chunk_size)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Example usage</span>
</span></span><span style="display:flex;"><span>chunked_section <span style="color:#f92672">=</span> chunk_text(long_text_section)
</span></span></code></pre></div><h4 id="3-transform-text-data-to-embeddings">3. Transform text data to embeddings<a hidden class="anchor" aria-hidden="true" href="#3-transform-text-data-to-embeddings">#</a></h4>
<p>Embeddings turn text into a numerical format that&rsquo;s understandable by LLMs. We&rsquo;ll use the Instructorüë®‚Äçüè´ model for its performance:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> sentence_transformers <span style="color:#f92672">import</span> SentenceTransformer
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> SentenceTransformer(<span style="color:#e6db74">&#39;hkunlp/instructor-large&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">embed_text</span>(text):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> model<span style="color:#f92672">.</span>encode(text)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Example usage</span>
</span></span><span style="display:flex;"><span>embedded_text <span style="color:#f92672">=</span> embed_text(<span style="color:#e6db74">&#34;Your text here&#34;</span>)
</span></span></code></pre></div><blockquote>
<p><strong>Note:</strong> While we start with embeddings for efficiency, exploring LLMs for the top k chunks and reranking with tools like Cohere Rerank could further refine relevance.</p>
</blockquote>
<h4 id="4-store-the-data-in-a-vectordb">4. Store the data in a vectorDB<a hidden class="anchor" aria-hidden="true" href="#4-store-the-data-in-a-vectordb">#</a></h4>
<p>We&rsquo;ll use pgvector for storage. Here‚Äôs how to set it up with PostgreSQL:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-sql" data-lang="sql"><span style="display:flex;"><span><span style="color:#66d9ef">CREATE</span> EXTENSION pgvector;
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">CREATE</span> <span style="color:#66d9ef">TABLE</span> documents (id SERIAL <span style="color:#66d9ef">PRIMARY</span> <span style="color:#66d9ef">KEY</span>, text_vector vector(<span style="color:#ae81ff">512</span>));
</span></span></code></pre></div><p>And inserting data:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> psycopg2
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Connect to your PostgreSQL database</span>
</span></span><span style="display:flex;"><span>conn <span style="color:#f92672">=</span> psycopg2<span style="color:#f92672">.</span>connect(<span style="color:#e6db74">&#34;dbname=your_db user=your_user&#34;</span>)
</span></span><span style="display:flex;"><span>cur <span style="color:#f92672">=</span> conn<span style="color:#f92672">.</span>cursor()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Insert the embedded text</span>
</span></span><span style="display:flex;"><span>cur<span style="color:#f92672">.</span>execute(<span style="color:#e6db74">&#34;INSERT INTO documents (text_vector) VALUES (vector(</span><span style="color:#e6db74">%s</span><span style="color:#e6db74">))&#34;</span>, (embedded_text,))
</span></span><span style="display:flex;"><span>conn<span style="color:#f92672">.</span>commit()
</span></span></code></pre></div><h4 id="5-perform-inference">5. Perform inference<a hidden class="anchor" aria-hidden="true" href="#5-perform-inference">#</a></h4>
<p>For inference, we retrieve relevant documents, optionally rerank them, and compile a contextual prompt:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">query_vector_db</span>(query_embedding, top_k<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Query the top_k most similar vectors</span>
</span></span><span style="display:flex;"><span>    cur<span style="color:#f92672">.</span>execute(<span style="color:#e6db74">&#34;SELECT id, text_vector &lt;-&gt; vector(</span><span style="color:#e6db74">%s</span><span style="color:#e6db74">) AS distance FROM documents ORDER BY distance ASC LIMIT </span><span style="color:#e6db74">%s</span><span style="color:#e6db74">&#34;</span>, (query_embedding, top_k))
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> cur<span style="color:#f92672">.</span>fetchall()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Example: embedding a query and retrieving related documents</span>
</span></span><span style="display:flex;"><span>query_embedding <span style="color:#f92672">=</span> embed_text(<span style="color:#e6db74">&#34;Your query here&#34;</span>)
</span></span><span style="display:flex;"><span>related_documents <span style="color:#f92672">=</span> query_vector_db(query_embedding)
</span></span></code></pre></div><h3 id="conclusion">Conclusion<a hidden class="anchor" aria-hidden="true" href="#conclusion">#</a></h3>
<p>Building a RAG-based LLM from scratch teaches you the nuts and bolts of modern NLP applications, from data preprocessing to deploying scalable solutions. This guide is just the beginning; stay tuned for more in-depth exploration of fine-tuning and prompt engineering techniques.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2024 <a href="https://blog.lays.pro./">Cyril LAY</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
