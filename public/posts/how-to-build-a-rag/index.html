<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Develop a Retrieval Augmented Generation (RAG) LLM Application | Cyril LAY</title>
<meta name="keywords" content="">
<meta name="description" content="This article explains how to customize a large language model for your specific needs.
There are many approaches to adapt a Large Language Model (LLM) for your data, with the goal of enhancing the responses relevance.
Prompt engineering : you spend time working on a good prompt, eventually a templated prompt where you can include your data. This is the fastest way, but will also quickly become limited, as the context size (maximum input you can give to a model) is usually in the range of 4 - 16K tokens (3 - 11K words), and it is shared between question and response.">
<meta name="author" content="Cyril LAY">
<link rel="canonical" href="https://blog.lays.pro./posts/how-to-build-a-rag/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.5ff2630c4d1b3e25bc21f0ecd96681dbcf58219e741fa627857820b5485cb770.css" integrity="sha256-X/JjDE0bPiW8IfDs2WaB289YIZ50H6YnhXggtUhct3A=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://blog.lays.pro./favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://blog.lays.pro./favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://blog.lays.pro./favicon-32x32.png">
<link rel="apple-touch-icon" href="https://blog.lays.pro./apple-touch-icon.png">
<link rel="mask-icon" href="https://blog.lays.pro./safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript><meta property="og:title" content="Develop a Retrieval Augmented Generation (RAG) LLM Application" />
<meta property="og:description" content="This article explains how to customize a large language model for your specific needs.
There are many approaches to adapt a Large Language Model (LLM) for your data, with the goal of enhancing the responses relevance.
Prompt engineering : you spend time working on a good prompt, eventually a templated prompt where you can include your data. This is the fastest way, but will also quickly become limited, as the context size (maximum input you can give to a model) is usually in the range of 4 - 16K tokens (3 - 11K words), and it is shared between question and response." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://blog.lays.pro./posts/how-to-build-a-rag/" /><meta property="og:image" content="https://blog.lays.pro./assets/images/mixtral.png"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-02-06T23:09:33+01:00" />
<meta property="article:modified_time" content="2024-02-06T23:09:33+01:00" /><meta property="og:site_name" content="AI &amp; Data blog by Cyril LAY" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://blog.lays.pro./assets/images/mixtral.png"/>

<meta name="twitter:title" content="Develop a Retrieval Augmented Generation (RAG) LLM Application"/>
<meta name="twitter:description" content="This article explains how to customize a large language model for your specific needs.
There are many approaches to adapt a Large Language Model (LLM) for your data, with the goal of enhancing the responses relevance.
Prompt engineering : you spend time working on a good prompt, eventually a templated prompt where you can include your data. This is the fastest way, but will also quickly become limited, as the context size (maximum input you can give to a model) is usually in the range of 4 - 16K tokens (3 - 11K words), and it is shared between question and response."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://blog.lays.pro./posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Develop a Retrieval Augmented Generation (RAG) LLM Application",
      "item": "https://blog.lays.pro./posts/how-to-build-a-rag/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Develop a Retrieval Augmented Generation (RAG) LLM Application",
  "name": "Develop a Retrieval Augmented Generation (RAG) LLM Application",
  "description": "This article explains how to customize a large language model for your specific needs.\nThere are many approaches to adapt a Large Language Model (LLM) for your data, with the goal of enhancing the responses relevance.\nPrompt engineering : you spend time working on a good prompt, eventually a templated prompt where you can include your data. This is the fastest way, but will also quickly become limited, as the context size (maximum input you can give to a model) is usually in the range of 4 - 16K tokens (3 - 11K words), and it is shared between question and response.",
  "keywords": [
    
  ],
  "articleBody": "This article explains how to customize a large language model for your specific needs.\nThere are many approaches to adapt a Large Language Model (LLM) for your data, with the goal of enhancing the responses relevance.\nPrompt engineering : you spend time working on a good prompt, eventually a templated prompt where you can include your data. This is the fastest way, but will also quickly become limited, as the context size (maximum input you can give to a model) is usually in the range of 4 - 16K tokens (3 - 11K words), and it is shared between question and response. So you won‚Äôt be able to include all of your data in the prompt (or you will do so at the expense of the response length)\nFine-tuning : you can take existing models and fine-tune them with your own data. You could use openAI‚Äôs fine-tuning API (can become expensive if you don‚Äôt get it right the first time), or fine-tune an open-source model, like mistral, mixtral, llama2. This approach yields great results, but is still expensive in terms of engineering resources and computing costs, as one needs to go through model training iteration loops, harder than iterating over a prompt. More information on when to fine-tune here.\nTraining from scratch : this is so prohibitively expensive that only companies like OpenAI, Meta, Google can afford the 1000s of GPUs required to perform what we call the pretraining (understand basic language and concepts by learning to predict the next token)\nRetrieval Augmented generation (RAG) : it is a good trade off with medium engineering complexity and costs, and high impact on the accuracy and relevance of the augmented model. In a way, it‚Äôs a bit like prompt engineering on steroids. Let‚Äôs deep dive into it and see how to make one.\nRAG is a technique for enhancing the accuracy, reliability and relevance of generative AI models by giving it ultra relevant facts at query time, previously fetched from internal or external sources and vectorized as embeddings.\nHere is how such a system can be built :\nLet‚Äôs dive into building a RAG-based LLM application using publicly available data.\nWhat we‚Äôll cover: üíª Setting up and processing workloads locally. üöÄ Scaling your solution to the cloud effortlessly. üí° Enhancing performance with reranking, fine-tuning, and prompt engineering. Steps 1. Get the data Start by gathering data from public sources. For this project, we want to use web resources, this means crawling a web page (make sure the license allows it !) and collect a dataset like below.\nurl section text https://huggingface.co/docs/trl Introduction ‚ÄúHere‚Äôs how to start with LLM‚Ä¶‚Äù https://huggingface.co/docs/trl Getting Data ‚ÄúData is crucial for training LLMs‚Ä¶‚Äù https://docs.metaflow.org/api/client Training ‚ÄúTo train your model, begin with‚Ä¶‚Äù ‚Ä¶ ‚Ä¶ ‚Ä¶ text should be about 1000 characters.\nFuture articles will delve into using internal/proprietary databases.\n2. Preprocess the data You‚Äôre probably gonna get a dirty dataset, with many lines without any content, or a very limited content. Add columns to your dataframe for the word count, to perform basic filtering and remove rows without info :\ndf = df[df.word_count \u003e 15] df = df[df.char_count \u003e 50] 3. Transform text data to embeddings Embeddings turn text into a numerical vectors that are understandable by LLMs.\nWe‚Äôll use LLamaIndex for embedding and storing the vectors, so that‚Äôs where you select the model you want to use. We‚Äôll use openAI‚Äôs text-embedding-3-small which is cheap (1$ = 10K pages of text embedded) and efficient.\nfrom llama_index import ServiceContext, set_global_service_context, VectorStoreIndex set_global_service_context( ServiceContext.from_defaults(llm=\"gpt-3.5-turbo\", embed_model='text-embedding-3-small') ) Note: While we start with embeddings only for efficiency, adding a reranking model would further refine relevance. We‚Äôll do it as a next step\n4. Store the data in a vectorDB We‚Äôll use pgvector for storing our embeddings. Here‚Äôs how to set it up with PostgreSQL:\nCREATE EXTENSION pgvector; CREATE TABLE document ( id serial primary key, \"text\" text not null, source text not null, embedding vector(1024) ); And inserting your embedded vectors :\nfrom llama_index.indices.vector_store import VectorStoreIndex from llama_index.vector_stores import PGVectorStore vector_store = PGVectorStore.from_params( database=, host=, password=, port=, user=, table_name=\"document\", embed_dim=1536, # openai embedding dimension ) storage_context = StorageContext.from_defaults(vector_store=vector_store) index = VectorStoreIndex.from_documents( df, storage_context=storage_context, show_progress=True ) 5. Perform inference For inference, we need to :\nembed the user query (like we did for the documentation) use it on the vectorDB to retrieve relevant documents optionally rerank them with a reranker Compile a contextual prompt Note : we could also use LLamaIndex for those parts\ndef query_vector_db(query_embedding, top_k=5): # Query the top_k most similar vectors cur.execute(\"SELECT * FROM document ORDER BY embedding \u003c-\u003e %s LIMIT %s\", (query_embedding, top_k)) return cur.fetchall() query = \"How to fine-tune a mistral LLM with DPO ?\" query_embedding = openai.OpenAI().embeddings.create(input = query, model=\"text-embedding-3-small\").data[0].embedding related_documents = query_vector_db(query_embedding) # optional : rerank here Compile a prompt :\nprompt = f\"\"\" Answer this question using your existing knowledge and the relevant context below: ${related_documents[:3]} If the context doesn't help you answer, prefix your message with: \"Context not relevant.\" If the context is empty, prefix your message with: \"No context.\" If you don't know the answer and the context doesn't help you, say \"I don't know\". The question is : ${query} \"\"\" And finally send it to your LLM :\nfrom openai import OpenAI; client = OpenAI() stream = client.chat.completions.create( model=\"gpt-4\", messages=[{\"role\": \"user\", \"content\": prompt}], stream=True, ) for chunk in stream: if chunk.choices[0].delta.content is not None: print(chunk.choices[0].delta.content, end=\"\") Conclusion Building a RAG-based LLM from scratch teaches you the nuts and bolts of modern NLP applications, from data preprocessing to deploying scalable solutions. This guide is just the beginning; stay tuned for more in-depth exploration of RAG and fine-tuning techniques.\nTODO Homogenize between LlamaIndex or by hand ",
  "wordCount" : "942",
  "inLanguage": "en",
  "datePublished": "2024-02-06T23:09:33+01:00",
  "dateModified": "2024-02-06T23:09:33+01:00",
  "author":{
    "@type": "Person",
    "name": "Cyril LAY"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://blog.lays.pro./posts/how-to-build-a-rag/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Cyril LAY",
    "logo": {
      "@type": "ImageObject",
      "url": "https://blog.lays.pro./favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://blog.lays.pro./" accesskey="h" title="Cyril LAY (Alt + H)">Cyril LAY</a>
            <div class="logo-switches">
            </div>
        </div>
        <ul id="menu">
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Develop a Retrieval Augmented Generation (RAG) LLM Application
    </h1>
    <div class="post-meta"><span title='2024-02-06 23:09:33 +0100 CET'>February 6, 2024</span>&nbsp;¬∑&nbsp;Cyril LAY

</div>
  </header> 
  <div class="post-content"><p>This article explains how to customize a large language model for your specific needs.</p>
<p>There are many approaches to adapt a Large Language Model (LLM) for your data, with the goal of enhancing the responses relevance.</p>
<ol>
<li>
<p><strong>Prompt engineering</strong> : you spend time working on a good prompt, eventually a templated prompt where you
can include your data. This is the fastest way, but will also quickly become limited, as the context size
(maximum input you can give to a model) is usually in the range of 4 - 16K tokens (3 - 11K words), and it is shared between question
and response. So you won&rsquo;t be able to include all of your data in the prompt (or you will do so at the expense of the response length)</p>
</li>
<li>
<p><strong>Fine-tuning</strong> : you can take existing models and fine-tune them with your own data. You could use openAI&rsquo;s
fine-tuning API (can become expensive if you don&rsquo;t get it right the first time), or fine-tune an open-source model,
like mistral, mixtral, llama2. This approach yields great results, but is still expensive in terms of engineering
resources and computing costs, as one needs to go through model training iteration loops, harder than iterating over a prompt.
More information on when to fine-tune <a href="https://platform.openai.com/docs/guides/fine-tuning/when-to-use-fine-tuning">here</a>.</p>
</li>
<li>
<p><strong>Training from scratch</strong> : this is so prohibitively expensive that only companies like OpenAI, Meta, Google can afford
the 1000s of GPUs required to perform what we call the <strong>pretraining</strong> (understand basic language and concepts by learning to predict the next token)</p>
</li>
<li>
<p><strong>Retrieval Augmented generation (RAG)</strong> : it is a good trade off with medium engineering complexity and costs,
and high impact on the accuracy and relevance of the augmented model. In a way, it&rsquo;s a bit like prompt engineering on steroids.
Let&rsquo;s deep dive into it and see how to make one.</p>
</li>
</ol>
<p>RAG is a technique for enhancing the accuracy, reliability and relevance of generative AI models by giving
it ultra relevant facts at query time, previously fetched from internal or external sources and vectorized as embeddings.</p>
<p>Here is how such a system can be built :</p>
<p><img loading="lazy" src="/rag-archi.png" alt="RAG architecture"  />
</p>
<p>Let&rsquo;s dive into building a RAG-based LLM application using publicly available data.</p>
<h3 id="what-well-cover">What we&rsquo;ll cover:<a hidden class="anchor" aria-hidden="true" href="#what-well-cover">#</a></h3>
<ul>
<li>üíª Setting up and processing workloads locally.</li>
<li>üöÄ Scaling your solution to the cloud effortlessly.</li>
<li>üí° Enhancing performance with reranking, fine-tuning, and prompt engineering.</li>
</ul>
<h3 id="steps">Steps<a hidden class="anchor" aria-hidden="true" href="#steps">#</a></h3>
<h4 id="1-get-the-data">1. Get the data<a hidden class="anchor" aria-hidden="true" href="#1-get-the-data">#</a></h4>
<p>Start by gathering data from public sources. For this project, we want to use web resources, this means
crawling a web page (make sure the license allows it !) and collect a dataset like below.</p>
<table>
<thead>
<tr>
<th>url</th>
<th>section</th>
<th>text</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://huggingface.co/docs/trl">https://huggingface.co/docs/trl</a></td>
<td>Introduction</td>
<td>&ldquo;Here&rsquo;s how to start with LLM&hellip;&rdquo;</td>
</tr>
<tr>
<td><a href="https://huggingface.co/docs/trl">https://huggingface.co/docs/trl</a></td>
<td>Getting Data</td>
<td>&ldquo;Data is crucial for training LLMs&hellip;&rdquo;</td>
</tr>
<tr>
<td><a href="https://docs.metaflow.org/api/client">https://docs.metaflow.org/api/client</a></td>
<td>Training</td>
<td>&ldquo;To train your model, begin with&hellip;&rdquo;</td>
</tr>
<tr>
<td>&hellip;</td>
<td>&hellip;</td>
<td>&hellip;</td>
</tr>
</tbody>
</table>
<p><code>text</code> should be about 1000 characters.</p>
<p>Future articles will delve into using internal/proprietary databases.</p>
<h4 id="2-preprocess-the-data">2. Preprocess the data<a hidden class="anchor" aria-hidden="true" href="#2-preprocess-the-data">#</a></h4>
<p>You&rsquo;re probably gonna get a dirty dataset, with many lines without any content, or a very limited content.
Add columns to your dataframe for the word count, to perform basic filtering and remove rows without info :</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>df <span style="color:#f92672">=</span> df[df<span style="color:#f92672">.</span>word_count <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">15</span>]
</span></span><span style="display:flex;"><span>df <span style="color:#f92672">=</span> df[df<span style="color:#f92672">.</span>char_count <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">50</span>]
</span></span></code></pre></div><h4 id="3-transform-text-data-to-embeddings">3. Transform text data to embeddings<a hidden class="anchor" aria-hidden="true" href="#3-transform-text-data-to-embeddings">#</a></h4>
<p>Embeddings turn text into a numerical vectors that are understandable by LLMs.</p>
<p>We&rsquo;ll use LLamaIndex for embedding and storing the vectors, so that&rsquo;s where you select the model you want to use. We&rsquo;ll use openAI&rsquo;s
<code>text-embedding-3-small</code> which is cheap (1$ = 10K pages of text embedded) and efficient.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> llama_index <span style="color:#f92672">import</span> ServiceContext, set_global_service_context, VectorStoreIndex
</span></span><span style="display:flex;"><span>set_global_service_context(
</span></span><span style="display:flex;"><span>    ServiceContext<span style="color:#f92672">.</span>from_defaults(llm<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;gpt-3.5-turbo&#34;</span>, embed_model<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;text-embedding-3-small&#39;</span>)
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><blockquote>
<p><strong>Note:</strong> While we start with embeddings only for efficiency,
adding a <a href="https://blog.llamaindex.ai/boosting-rag-picking-the-best-embedding-reranker-models-42d079022e83">reranking model</a>
would further refine relevance. We&rsquo;ll do it as a next step</p>
</blockquote>
<h4 id="4-store-the-data-in-a-vectordb">4. Store the data in a vectorDB<a hidden class="anchor" aria-hidden="true" href="#4-store-the-data-in-a-vectordb">#</a></h4>
<p>We&rsquo;ll use <a href="https://github.com/pgvector/pgvector">pgvector</a> for storing our embeddings.
Here‚Äôs how to set it up with PostgreSQL:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-sql" data-lang="sql"><span style="display:flex;"><span><span style="color:#66d9ef">CREATE</span> EXTENSION pgvector;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">CREATE</span> <span style="color:#66d9ef">TABLE</span> document (
</span></span><span style="display:flex;"><span>    id serial <span style="color:#66d9ef">primary</span> <span style="color:#66d9ef">key</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;text&#34;</span> text <span style="color:#66d9ef">not</span> <span style="color:#66d9ef">null</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">source</span> text <span style="color:#66d9ef">not</span> <span style="color:#66d9ef">null</span>,
</span></span><span style="display:flex;"><span>    embedding vector(<span style="color:#ae81ff">1024</span>)
</span></span><span style="display:flex;"><span>);
</span></span></code></pre></div><p>And inserting your embedded vectors :</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> llama_index.indices.vector_store <span style="color:#f92672">import</span> VectorStoreIndex
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> llama_index.vector_stores <span style="color:#f92672">import</span> PGVectorStore
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>vector_store <span style="color:#f92672">=</span> PGVectorStore<span style="color:#f92672">.</span>from_params(
</span></span><span style="display:flex;"><span>    database<span style="color:#f92672">=</span>,
</span></span><span style="display:flex;"><span>    host<span style="color:#f92672">=</span>,
</span></span><span style="display:flex;"><span>    password<span style="color:#f92672">=</span>,
</span></span><span style="display:flex;"><span>    port<span style="color:#f92672">=</span>,
</span></span><span style="display:flex;"><span>    user<span style="color:#f92672">=</span>,
</span></span><span style="display:flex;"><span>    table_name<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;document&#34;</span>,
</span></span><span style="display:flex;"><span>    embed_dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1536</span>,  <span style="color:#75715e"># openai embedding dimension</span>
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>storage_context <span style="color:#f92672">=</span> StorageContext<span style="color:#f92672">.</span>from_defaults(vector_store<span style="color:#f92672">=</span>vector_store)
</span></span><span style="display:flex;"><span>index <span style="color:#f92672">=</span> VectorStoreIndex<span style="color:#f92672">.</span>from_documents(
</span></span><span style="display:flex;"><span>    df, storage_context<span style="color:#f92672">=</span>storage_context, show_progress<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><h4 id="5-perform-inference">5. Perform inference<a hidden class="anchor" aria-hidden="true" href="#5-perform-inference">#</a></h4>
<p>For inference, we need to :</p>
<ul>
<li>embed the user query (like we did for the documentation)</li>
<li>use it on the vectorDB to retrieve relevant documents</li>
<li>optionally rerank them with a reranker</li>
<li>Compile a contextual prompt</li>
</ul>
<p>Note : we could also use LLamaIndex for those parts</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">query_vector_db</span>(query_embedding, top_k<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Query the top_k most similar vectors</span>
</span></span><span style="display:flex;"><span>    cur<span style="color:#f92672">.</span>execute(<span style="color:#e6db74">&#34;SELECT * FROM document ORDER BY embedding &lt;-&gt; </span><span style="color:#e6db74">%s</span><span style="color:#e6db74"> LIMIT </span><span style="color:#e6db74">%s</span><span style="color:#e6db74">&#34;</span>, (query_embedding, top_k))
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> cur<span style="color:#f92672">.</span>fetchall()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>query <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;How to fine-tune a mistral LLM with DPO ?&#34;</span>
</span></span><span style="display:flex;"><span>query_embedding <span style="color:#f92672">=</span> openai<span style="color:#f92672">.</span>OpenAI()<span style="color:#f92672">.</span>embeddings<span style="color:#f92672">.</span>create(input <span style="color:#f92672">=</span> query, model<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;text-embedding-3-small&#34;</span>)<span style="color:#f92672">.</span>data[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>embedding
</span></span><span style="display:flex;"><span>related_documents <span style="color:#f92672">=</span> query_vector_db(query_embedding)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># optional : rerank here</span>
</span></span></code></pre></div><p>Compile a prompt :</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>prompt <span style="color:#f92672">=</span> <span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    Answer this question using your existing knowledge and the relevant context below: $</span><span style="color:#e6db74">{</span>related_documents[:<span style="color:#ae81ff">3</span>]<span style="color:#e6db74">}</span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    If the context doesn&#39;t help you answer, prefix your message with: &#34;Context not relevant.&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    If the context is empty, prefix your message with: &#34;No context.&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    If you don&#39;t know the answer and the context doesn&#39;t help you, say &#34;I don&#39;t know&#34;.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    The question is : $</span><span style="color:#e6db74">{</span>query<span style="color:#e6db74">}</span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">&#34;&#34;&#34;</span>
</span></span></code></pre></div><p>And finally send it to your LLM :</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> openai <span style="color:#f92672">import</span> OpenAI; client <span style="color:#f92672">=</span> OpenAI()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>stream <span style="color:#f92672">=</span> client<span style="color:#f92672">.</span>chat<span style="color:#f92672">.</span>completions<span style="color:#f92672">.</span>create(
</span></span><span style="display:flex;"><span>    model<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;gpt-4&#34;</span>,
</span></span><span style="display:flex;"><span>    messages<span style="color:#f92672">=</span>[{<span style="color:#e6db74">&#34;role&#34;</span>: <span style="color:#e6db74">&#34;user&#34;</span>, <span style="color:#e6db74">&#34;content&#34;</span>: prompt}],
</span></span><span style="display:flex;"><span>    stream<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>,
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> chunk <span style="color:#f92672">in</span> stream:
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> chunk<span style="color:#f92672">.</span>choices[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>delta<span style="color:#f92672">.</span>content <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> <span style="color:#66d9ef">None</span>:
</span></span><span style="display:flex;"><span>        print(chunk<span style="color:#f92672">.</span>choices[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>delta<span style="color:#f92672">.</span>content, end<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;&#34;</span>)
</span></span></code></pre></div><h3 id="conclusion">Conclusion<a hidden class="anchor" aria-hidden="true" href="#conclusion">#</a></h3>
<p>Building a RAG-based LLM from scratch teaches you the nuts and bolts of modern NLP applications,
from data preprocessing to deploying scalable solutions.
This guide is just the beginning; stay tuned for more in-depth exploration of RAG and fine-tuning techniques.</p>
<h3 id="todo">TODO<a hidden class="anchor" aria-hidden="true" href="#todo">#</a></h3>
<ul>
<li>Homogenize between LlamaIndex or by hand</li>
</ul>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2024 <a href="https://blog.lays.pro./">Cyril LAY</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
