<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Journey into building a custom RAG (Day 1) | Cyril LAY</title>
<meta name="keywords" content="">
<meta name="description" content="I am currently building a RAG (Retrieval Augmented Generation) LLM application, and recently decided to share my journey on this blog.
I skip the intro and jump straight to my current status. Here&rsquo;s where I am:
Starting Point: To get started with all the boilerplate code, I used the Metaflow RAG demo repository and this anyScale blog post for insights and second point of view. Metaflow&rsquo;s repo include a streamlit chat webapp, utilities for scraping and parsing markdown documentation files, tools to retrieve embedded documents to augment the user prompt, etc.">
<meta name="author" content="Cyril LAY">
<link rel="canonical" href="https://blog.lays.pro./posts/building-a-rag-part-1/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.5ff2630c4d1b3e25bc21f0ecd96681dbcf58219e741fa627857820b5485cb770.css" integrity="sha256-X/JjDE0bPiW8IfDs2WaB289YIZ50H6YnhXggtUhct3A=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://blog.lays.pro./favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://blog.lays.pro./favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://blog.lays.pro./favicon-32x32.png">
<link rel="apple-touch-icon" href="https://blog.lays.pro./apple-touch-icon.png">
<link rel="mask-icon" href="https://blog.lays.pro./safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript><meta property="og:title" content="Journey into building a custom RAG (Day 1)" />
<meta property="og:description" content="I am currently building a RAG (Retrieval Augmented Generation) LLM application, and recently decided to share my journey on this blog.
I skip the intro and jump straight to my current status. Here&rsquo;s where I am:
Starting Point: To get started with all the boilerplate code, I used the Metaflow RAG demo repository and this anyScale blog post for insights and second point of view. Metaflow&rsquo;s repo include a streamlit chat webapp, utilities for scraping and parsing markdown documentation files, tools to retrieve embedded documents to augment the user prompt, etc." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://blog.lays.pro./posts/building-a-rag-part-1/" /><meta property="og:image" content="https://blog.lays.pro./assets/images/mixtral.png"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-02-08T16:34:28+01:00" />
<meta property="article:modified_time" content="2024-02-08T16:34:28+01:00" /><meta property="og:site_name" content="AI &amp; Data blog by Cyril LAY" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://blog.lays.pro./assets/images/mixtral.png"/>

<meta name="twitter:title" content="Journey into building a custom RAG (Day 1)"/>
<meta name="twitter:description" content="I am currently building a RAG (Retrieval Augmented Generation) LLM application, and recently decided to share my journey on this blog.
I skip the intro and jump straight to my current status. Here&rsquo;s where I am:
Starting Point: To get started with all the boilerplate code, I used the Metaflow RAG demo repository and this anyScale blog post for insights and second point of view. Metaflow&rsquo;s repo include a streamlit chat webapp, utilities for scraping and parsing markdown documentation files, tools to retrieve embedded documents to augment the user prompt, etc."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://blog.lays.pro./posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Journey into building a custom RAG (Day 1)",
      "item": "https://blog.lays.pro./posts/building-a-rag-part-1/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Journey into building a custom RAG (Day 1)",
  "name": "Journey into building a custom RAG (Day 1)",
  "description": "I am currently building a RAG (Retrieval Augmented Generation) LLM application, and recently decided to share my journey on this blog.\nI skip the intro and jump straight to my current status. Here\u0026rsquo;s where I am:\nStarting Point: To get started with all the boilerplate code, I used the Metaflow RAG demo repository and this anyScale blog post for insights and second point of view. Metaflow\u0026rsquo;s repo include a streamlit chat webapp, utilities for scraping and parsing markdown documentation files, tools to retrieve embedded documents to augment the user prompt, etc.",
  "keywords": [
    
  ],
  "articleBody": "I am currently building a RAG (Retrieval Augmented Generation) LLM application, and recently decided to share my journey on this blog.\nI skip the intro and jump straight to my current status. Here’s where I am:\nStarting Point: To get started with all the boilerplate code, I used the Metaflow RAG demo repository and this anyScale blog post for insights and second point of view. Metaflow’s repo include a streamlit chat webapp, utilities for scraping and parsing markdown documentation files, tools to retrieve embedded documents to augment the user prompt, etc. Current Status I replicated what was done on Metaflow’s blog, augmented with Metaflow’s docs. Now, I want to do one with Hugging Face’s docs as well, focusing on three libraries: transformers, accelerate, TRL (Transfer Reinforcement Learning). It works, but, it’s not amazing for now : Make iteration cycles cheap Currently, the pipeline uses openAI text-embedding-ada-002 (default from LLamaIndex) API for embeddings.\nAs you can see, there is now a text-embedding-3-small by OpenAI. It’s about 1/10th of the price, and is more performant : double win. Let’s use it :\nfrom llama_index import ServiceContext, set_global_service_context from llama_index.llms import OpenAI ServiceContext.from_defaults(llm=\"gpt-3.5-turbo\", embed_model=OpenAIEmbedding(model='text-embedding-3-small')) set_global_service_context(service_context) You can also choose any HuggingFace’s model to do the embeddings locally on your computer. One of the best at the time of writing is BAAI/bge-large-en-v1.5. You must tell LLamaIndex that it’s gonna be a local model with local: :\nServiceContext.from_defaults(llm=\"gpt-3.5-turbo\", embed_model='local:BAAI/bge-large-en-v1.5') I kept the LLM as GPT-3.5 for now, as it’s cheaper, and I’m mostly testing the relevance of the context augmentation at this stage, not the model itself.\nWhile doing this modification, I found the prompt used in the initial RAG repo.\nPrompt modification It’s the prompt used to include the context from the vectorDB, and then ask the question to the LLM :\nprompt = \"\"\" Answer this question only if there is relevant context below: {} If there is nothing in the context say: \"Could not find relevant context.\" Here is the retrieved context: {} \"\"\" I changed it to make it more flexible, I think it will help me in debugging and evaluating the pipeline.\nprompt = \"\"\" Answer this question using your existing knowledge the relevant context below: {} If the context doesn't help you answer, prefix your message with: \"Context not relevant.\" If the context is empty, prefix your message with: \"No context.\" If you don't know the answer and the context doesn't help you, say \"I don't know\". Here is the retrieved context: {} \"\"\" Next Steps Check if embeddings are saved between runs and see what’s the default strategy when iterating on the pipeline Run metaflow UI locally to easily track experiments Consider switching to a proper vectorDB See you for day 2 !\n",
  "wordCount" : "455",
  "inLanguage": "en",
  "datePublished": "2024-02-08T16:34:28+01:00",
  "dateModified": "2024-02-08T16:34:28+01:00",
  "author":{
    "@type": "Person",
    "name": "Cyril LAY"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://blog.lays.pro./posts/building-a-rag-part-1/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Cyril LAY",
    "logo": {
      "@type": "ImageObject",
      "url": "https://blog.lays.pro./favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://blog.lays.pro./" accesskey="h" title="Cyril LAY (Alt + H)">Cyril LAY</a>
            <div class="logo-switches">
            </div>
        </div>
        <ul id="menu">
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Journey into building a custom RAG (Day 1)
    </h1>
    <div class="post-meta"><span title='2024-02-08 16:34:28 +0100 CET'>February 8, 2024</span>&nbsp;·&nbsp;Cyril LAY

</div>
  </header> 
  <div class="post-content"><p>I am currently building a <a href="https://blog.lays.pro/posts/how-to-build-a-rag/">RAG (Retrieval Augmented Generation)</a> LLM application, and recently
decided to share my journey on this blog.<br>
I skip the intro and jump straight to my current status. Here&rsquo;s where I am:</p>
<ul>
<li><strong>Starting Point</strong>: To get started with all the boilerplate code, I used the
<a href="https://github.com/outerbounds/rag-demo">Metaflow RAG demo repository</a> and this
<a href="https://www.anyscale.com/blog/a-comprehensive-guide-for-building-rag-based-llm-applications-part-1">anyScale blog post</a>
for insights and second point of view.
Metaflow&rsquo;s repo include a streamlit chat webapp, utilities for scraping and parsing markdown documentation files,
tools to retrieve embedded documents to augment the user prompt, etc.</li>
</ul>
<h3 id="current-status">Current Status<a hidden class="anchor" aria-hidden="true" href="#current-status">#</a></h3>
<ul>
<li>I replicated what was done on Metaflow’s blog, augmented with Metaflow’s docs.
Now, I want to do one with Hugging Face’s docs as well, focusing on three libraries: <a href="https://huggingface.co/docs/transformers/index">transformers</a>,
<a href="https://huggingface.co/docs/accelerate">accelerate</a>, <a href="https://huggingface.co/docs/trl/">TRL</a> (Transfer Reinforcement Learning).</li>
<li>It works, but, it&rsquo;s not amazing for now :</li>
</ul>
<p><img loading="lazy" src="/rag-q1.png" alt="rag-q1"  />

<img loading="lazy" src="/rag-q2.png" alt="rag-q2"  />

<img loading="lazy" src="/rag-q3.png" alt="rag-q3"  />
</p>
<!-- raw HTML omitted -->
<h3 id="make-iteration-cycles-cheap">Make iteration cycles cheap<a hidden class="anchor" aria-hidden="true" href="#make-iteration-cycles-cheap">#</a></h3>
<p>Currently, the pipeline uses openAI <code>text-embedding-ada-002</code> (default from LLamaIndex) API for embeddings.</p>
<p><img loading="lazy" src="/openai-embedding-price.png" alt="OpenAI Embeddings API prices"  />
</p>
<p>As you can see, there is now a <code>text-embedding-3-small</code> by OpenAI. It&rsquo;s about 1/10th of the price, and is
more performant : double win. Let&rsquo;s use it :</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> llama_index <span style="color:#f92672">import</span> ServiceContext, set_global_service_context
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> llama_index.llms <span style="color:#f92672">import</span> OpenAI
</span></span><span style="display:flex;"><span>ServiceContext<span style="color:#f92672">.</span>from_defaults(llm<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;gpt-3.5-turbo&#34;</span>, embed_model<span style="color:#f92672">=</span>OpenAIEmbedding(model<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;text-embedding-3-small&#39;</span>))
</span></span><span style="display:flex;"><span>set_global_service_context(service_context)
</span></span></code></pre></div><p>You can also choose any HuggingFace&rsquo;s model to do the embeddings locally on your computer. One of the best
at the time of writing is <code>BAAI/bge-large-en-v1.5</code>. You must tell LLamaIndex that it&rsquo;s gonna be a local model with <code>local:</code> :</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>ServiceContext<span style="color:#f92672">.</span>from_defaults(llm<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;gpt-3.5-turbo&#34;</span>, embed_model<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;local:BAAI/bge-large-en-v1.5&#39;</span>)
</span></span></code></pre></div><p>I kept the LLM as GPT-3.5 for now, as it&rsquo;s cheaper, and I’m mostly testing the relevance of the context augmentation at this stage, not the model itself.</p>
<p>While doing this modification, I found the prompt used in the initial RAG repo.</p>
<h3 id="prompt-modification">Prompt modification<a hidden class="anchor" aria-hidden="true" href="#prompt-modification">#</a></h3>
<p>It&rsquo;s the prompt used to include the context from the vectorDB, and then ask the question to the LLM :</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>prompt <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">     Answer this question only if there is relevant context below: </span><span style="color:#e6db74">{}</span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">     If there is nothing in the context say: &#34;Could not find relevant context.&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">     Here is the retrieved context: </span><span style="color:#e6db74">{}</span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74"> &#34;&#34;&#34;</span>
</span></span></code></pre></div><p>I changed it to make it more flexible, I think it will help me in debugging and evaluating the pipeline.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>prompt <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    Answer this question using your existing knowledge the relevant context below: </span><span style="color:#e6db74">{}</span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    If the context doesn&#39;t help you answer, prefix your message with: &#34;Context not relevant.&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    If the context is empty, prefix your message with: &#34;No context.&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    If you don&#39;t know the answer and the context doesn&#39;t help you, say &#34;I don&#39;t know&#34;.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    Here is the retrieved context: </span><span style="color:#e6db74">{}</span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">&#34;&#34;&#34;</span>
</span></span></code></pre></div><h3 id="next-steps">Next Steps<a hidden class="anchor" aria-hidden="true" href="#next-steps">#</a></h3>
<ul>
<li>Check if embeddings are saved between runs and see what&rsquo;s the default strategy when iterating on the pipeline</li>
<li>Run metaflow UI locally to easily track experiments</li>
<li>Consider switching to a proper vectorDB</li>
</ul>
<p>See you for day 2 !</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2024 <a href="https://blog.lays.pro./">Cyril LAY</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
