<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Cyril LAY</title>
    <link>https://blog.lays.pro./</link>
    <description>Recent content on Cyril LAY</description>
    <image>
      <title>Cyril LAY</title>
      <url>https://blog.lays.pro./assets/images/mixtral.png</url>
      <link>https://blog.lays.pro./assets/images/mixtral.png</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 08 Feb 2024 16:34:28 +0100</lastBuildDate>
    <atom:link href="https://blog.lays.pro./index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Journey into building a custom RAG (Day 1)</title>
      <link>https://blog.lays.pro./posts/building-a-rag-part-1/</link>
      <pubDate>Thu, 08 Feb 2024 16:34:28 +0100</pubDate>
      <guid>https://blog.lays.pro./posts/building-a-rag-part-1/</guid>
      <description>I am currently building a RAG (Retrieval Augmented Generation) LLM application, and recently decided to share my journey on this blog.
I skip the intro and jump straight to my current status. Here&amp;rsquo;s where I am:
Starting Point: To get started with all the boilerplate code, I used the Metaflow RAG demo repository and this anyScale blog post for insights and second point of view. Metaflow&amp;rsquo;s repo include a streamlit chat webapp, utilities for scraping and parsing markdown documentation files, tools to retrieve embedded documents to augment the user prompt, etc.</description>
    </item>
    <item>
      <title>Develop a Retrieval Augmented Generation (RAG) LLM Application</title>
      <link>https://blog.lays.pro./posts/how-to-build-a-rag/</link>
      <pubDate>Tue, 06 Feb 2024 23:09:33 +0100</pubDate>
      <guid>https://blog.lays.pro./posts/how-to-build-a-rag/</guid>
      <description>This article explains how to customize a large language model for your specific needs.
There are many approaches to adapt a Large Language Model (LLM) for your data, with the goal of enhancing the responses relevance.
Prompt engineering : you spend time working on a good prompt, eventually a templated prompt where you can include your data. This is the fastest way, but will also quickly become limited, as the context size (maximum input you can give to a model) is usually in the range of 4 - 16K tokens (3 - 11K words), and it is shared between question and response.</description>
    </item>
    <item>
      <title>Detect Deep Fakes in 30 minutes with Computer Vision</title>
      <link>https://blog.lays.pro./posts/deep-fake-detection/</link>
      <pubDate>Sat, 01 Dec 2018 00:00:00 +0000</pubDate>
      <guid>https://blog.lays.pro./posts/deep-fake-detection/</guid>
      <description>In the realm of digital content, the rise of deep fakes has presented a unique set of challenges. I joined the Kaggle Deep Fake Detection Challenge (DFDC) and challenged myself to submit a working solution in two days maximum. I wrote my approach below. Code is here.
The Challenge at a Glance The dataset size is a challenge by itself, being a 500GB of video content dataset.
Approach and Methodology Frame Extraction: Capture a few frames from each video to immediately reduce the dataset size by 90%, and obtain something small to iterate with.</description>
    </item>
    <item>
      <title>Apache Spark Best Practices Megalist</title>
      <link>https://blog.lays.pro./posts/spark-best-practices/</link>
      <pubDate>Mon, 22 May 2017 00:00:00 +0000</pubDate>
      <guid>https://blog.lays.pro./posts/spark-best-practices/</guid>
      <description>Navigating through the complexities of Apache Spark job optimization can be challenging. Based on recent consulting experiences with various companies, I have compiled a list of best practices for optimizing Apache Spark applications. These strategies have proven to be effective in enhancing performance and cost-efficiency.
Partitioning Partition wisely: It&amp;rsquo;s important to balance the size of partitions. Avoid too many small partitions (big coordination overhead) and too few large ones (no parallelism).</description>
    </item>
    <item>
      <title>Detecting bot traffic on a webserver</title>
      <link>https://blog.lays.pro./posts/bot-detection/</link>
      <pubDate>Sun, 26 Feb 2017 23:18:58 +0100</pubDate>
      <guid>https://blog.lays.pro./posts/bot-detection/</guid>
      <description>TL;DR I did a basic bot detection POC/analysis on HTTP logs of a website, for an interview with a company in 2017. Here are the results :
Among the 68,000 unique IPs, 10% of them were classified as potential bots by our algorithm. Among the 5 million requests, 80% were classified as bot traffic. According to this article, 40% of the overall web traffic comes from bots. Given the simplicity of our algorithm, it is most probable that it flags a lot of false positives, as we will see very soon.</description>
    </item>
  </channel>
</rss>
